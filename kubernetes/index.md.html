<meta charset="utf-8">
                            **Elements of Kubernetes**
                            Stijn Heymans
                            published: N/A

# Introduction

I'll go over what I think are the elements of a kubernetes _from an application
developer's perspective_. A certain familiarity with deploying applications in
a production environment is expected (if you can imagine what things like
_downtime_, _rollback_, _production_ mean, you should be OK).  I'll do this by
assuming you have an application and there are a couple of requirements you
have on this application before you will be able to put it in production:

- You need to run many instances of your application to be able to serve traffic you have, which runs into the 1000s of request per second,
- Clients of your application will want to connect to one address, requests will be guided to different instances of your application automatically
- You need to be able to push changes to your application without downtime
- You need to be able to rollback changes to your application
- There is basic warm-up that needs to happen with your application before it can start serving up requests (for example, it needs to sync its data with data other instances of the application have available)
- Sometimes an instance of your application will die and stop serving requests, you have not figured out why, but you want that instance of the application to become invisible to clients of your application. Traffic should be guided to still functioning instances of your application
- To save costs during the night when you need less capability on your application, you'll want to reuse some of your machines that the instances of application normally run on to run batch jobs for maintenance
- You will have regular events (oh, say shopping events) during which the amount of machines to serve your traffic will not be enough, so you'll want to scale up the amount of machines you have available for your cluster. After the event, you'll want to get back to normal and scale down,

In comes Kubernetes (we will refer to _Kubernetes_ by its common abbreviation
of _k8s_ -- pronounced _kates_). While Kubernetes is massive hammer, it does
hammer the above problems squarely away. I dispariginaly say _hammer_ as it
indeed is able to do more than what you'd need. Its historic origin play a role
in that as used by a Cloud Provider in which many different applications run on
the same host of machines and they'd want to optimize usage of infrastructure.
In the use case we laid out, we are not _that_ interested in running different
applications on the same infrastructure, although the particular requirement to
save cost during the night and run some batch jobs on the same machine that
your machine runs on during the day, is designed to hint at that use.

The requirements as laid out above come from my personal experience, and the
most knowledge I have on k8s comes from [Kubernetes in
Action](https://www.goodreads.com/book/show/34013922-kubernetes-in-action) by
Marko Luk≈°a. The book goes in tremendous depth and is useful for infrastructure
administrators (which is a whole world on itself that I will barely touch on
here). I'll stick to what I know which is application development and what I
need to run an application in production satisfying the above requirements.

# Your Application and Running Many Instances of It

## A Containerized Application

For the application, I'll be running a simple web server that serves up the
sentence `Hi from HOST` where `HOST` will be the host it is running from. We're
in a brave new world, so this application has been packed up such that it can
run as a Docker container.

I've posted the source of this application on
[github](https://github.com/sheymans/elementsOf/tree/master/kubernetes/demo/hi_app)
and I've pushed the container image to the [Docker
Hub](https://hub.docker.com/) with name
[stijnh/hi_app](https://hub.docker.com/repository/docker/stijnh/hi).

If at this point, you feel slightly lost and it's related to this article, this
may be a good time to take a break from this article and read up on the
[Elements of Docker
Containers](http://www.stijnheymans.net/elements_of_docker_containers.html).

The container image is public so you can try out the app on your laptop right now with:

```
docker container run -d -p 8111:8111 -t stijnh/hi
```

Recall that `docker container run` indicates you're asking to run a container
image; `-d` indicates you're going to do that in the background as a daemon
(rather than interactive), `-p 8111:8111` means you're mapping your `localhost`
port 8111 (on your laptop) to port `8111` on your docker container behind which
your app is running, `-t stijnh/hi` indicates the image you're using to get
the container from.

At this point you can open your browser, and navigating to `localhost:8111`
should show you `Hi from fbd698569ec5` where `fbd698569ec5` will be different
of course for you as this the container's hostname.

Alright, at this point, I'll refer to this state as "you have 1 app
instance running". It's running as a container on your laptop. Your laptop in
this sense is the _machine_ (or if you feel like a rockstar, you're _bare metal
machine_).

## Running Many Instances of It

### Pod

We have 1 instance of our app running, but now you'll have thousands of users
saying hi, so you can't have only 1 instance -- that will not fit your
megalomanic dreams!  We need more! We need at least 3!

But first we need to define a bit better of what we need 3. We clearly need 3
containers (each container runs our app). As this is example is illustrating a
couple of elements of kubernetes, we've not mentioned real-world things like
logging. Typically, your application will be writing logs, and you'll need
something to _rotate_ these logs away to more permanent storage (to [AWS's S3](https://aws.amazon.com/s3/)
for example, where you could query it using
[Athena](https://aws.amazon.com/athena/)). We'd want this log writing in a
separate container (other apps could use it as well so it's not typical for
this app alone). You'd want these 2 containers always running
together[^sidecar], so if I say I want 3 instances of my app, I actually want 3
instances of my app with the logging container. Hence, the first level of
abstraction -- the base unit it deals with -- is _not_ a container, it's
something called a _pod_ in kubernetes. So let's do some more wrapping, and
wrap that container into a _Pod_ manifest[^manifest] that describes what the
pod should look like. If you recall `Dockerfile`s, then this is similar in
philosophy: in a `Dockerfile` you describe your app so that `docker` knows how
to build an image, in a _Pod_ manifest you describe what all goes in your
_Pod_, most importantly what container image to use such k8s knows how to
create a Pod.

Pods are usually defined in [YAML](https://en.wikipedia.org/wiki/YAML) files,
so let's do this. I have a file `hi-pod.yaml`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hi-pod
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

which is really doing nothing more than naming my pod `hi-pod` and indicating
that it's running the container
[`stijnh/hi`](https://hub.docker.com/repository/docker/stijnh/hi).

Let's go over the pod definition, line by line:

```
apiVersion: v1
```

Indicates what API version of kubernetes this kubernetes _resource_ is defined
in (_Pods_, together with a whole lot of other things, are named _resources_ in
k8s), in this case `v1`. I have no memory, nor patience for remembering API
versions, but there are [explanations of what version to use when out
there](https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html).



```
kind: Pod
```

The type of resource you're defining, a `Pod`.

```
metadata:
  name: hi-pod
```

Some metadata about the pod, in this case its name `hi-pod`.

```
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

The pod's specification, in this case a list of 1 container, where the
container is described by it's image tag `stijnh/hi`, given a name `hi`, and
indicating that the container exposes the port `8111`.

If you only remember one thing from the above, may it be that you'll see 4
parts of such a resource description which you'll see with other types of
resources as well: the `apiVersion`, the `kind`, the `metadata`, and the
`spec`.

With that said, can we deploy this pod now please? Yes. I'm going to assume
you're on your laptop and have something like
[minikube](https://github.com/kubernetes/minikube) installed. As I'm tackling
k8s from the perspective of an application developer, I'll not get into the
details of setting up a cluster that serves production traffic. 

This _is_ a good time to to talk about some general k8s concepts. But first, go
check out [minikube](https://github.com/kubernetes/minikube) and install it so
you can try out some stuff on your laptop.

Once you're back (you were never gone though, were you), you might have read
that minikube will give you a _single-node cluster_. You'll hear _cluster_ a
lot, so what is a cluster? My working definition is that it's a set of nodes.
What's a _node_ then? Again, my working definition is _that's a machine, an EC2
instance, a laptop, some kind of computor with a CPU and memory, an old Pentium
tower in a dusty basement, ..._ you get the idea. A typical mapping in my head
would be _if I have 40 EC2 instances in the EU region to serve traffic_, that's
a _cluster_ of _40_ nodes. 


Now that you have `minikube` installed, this command should show something:

```shell
$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   0d   v1.18.3
```

Your first use of `kubectl`, but not your last. `kubectl` is the command-line
tool that will allow you to interact with your cluster asking it about its
nodes, its pods, ...In this case, we know that we 1 node running, called
`minikube`. This is good to keep in mind, all example we'll show later will
involve 1 node. In reality, you'll be working with clusters that have many many
more nodes of course.

Alright, we have a cluster with 1 node. We have our `hi` app, tucked away in a
container, and we defined a pod that is supposed to run that container. Let's
deploy that pod using `kubectl`:

```shell
$ kubectl create -f hi_pod.yaml
pod/hi-pod created
```

Verify that your pod is created:

```shell
$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
hi-pod   1/1     Running   0          54s
```

This shows us that the pod is `Running` and has been doing so for `54s`. Note the levels of indirection: 

- when we ran the app `hi.js` we could just navigate to `localhost:8111` and see the `Hi from...`
- when we ran the app in a container, we needed to forward the localhost's port `8111` _into_ the container using `docker container run -d -p 8111:8111 -t stijnh/hi`
- when we run the app in a container in a pod, we need to do what?

We could forward ports via `kubectl` as well, but that would be mostly be for
debugging. We'll defer to answering this question to the next section where
we'll talk about how to expose your pods to other pods. For now, I'll just show how you can check the logs of that pod:

```
$ kubectl logs hi-pod
hi: listening on port 8111 of host hi-pod
```

That's indeed what the app logs. Note that the host is listed as `hi-pod` which
corresponds to the name of the pod.


You can more details on your pod by doing a `describe`:

```shell
$ kubectl describe pod hi-pod
```

There will be a lot of output, but interesting is for example the last section:

```shell
Events:
  Type    Reason     Age        From               Message
  ----    ------     ----       ----               -------
  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned default/hi-pod to minikube
  Normal  Pulling    23h        kubelet, minikube  Pulling image "stijnh/hi"
  Normal  Pulled     23h        kubelet, minikube  Successfully pulled image "stijnh/hi"
  Normal  Created    23h        kubelet, minikube  Created container hi
  Normal  Started    23h        kubelet, minikube  Started container hi
```

You'll see the container image `stijnh/hi` being pulled and then the container being created. Summarizing this with a picture, we have our container image, deployed in a pod on 1 node:

![Running 1 pod on your laptop](./diagrams/pod.svg)


### Deployment

Now that we have 1 instance of your application running (on 1 pod, in 1
container). We indicated as a requirement that _your application to be able to
serve traffic you have, which runs into the 1000s of request per second_ and
turns out 1 instance of your application is not able to do that. Assume each
application can serve 250 requests per second, and we need to be able to serve
1000 in total. Hence, we need 4 instances of the application running.

''OK, let me run 4 containers of my application'', you say. "yes, but no" I
reply. The unit of operation when your application is managed by k8s is a pod,
so we'll want 4 pods (each in turn running 1 container).

There's a kunbernetes resource type (the first one we met was a `Pod`) that
allows you to specify how many pods k8s should be keeping around: a
[`Deployment`](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). 

So that will be the core functionality of a `Deployment` that I'm interested
in at this point, specify how many of a certain pod I'd like around. As with
pods, deployments are specified using YAML manifests and the line that will get
me my 4 pods is:

```
spec:
  replicas: 3
```

Glad to see you're awake Grashopper, yeah 4 of course:

```
replicas: 4
```

How will you describe of _which pod you want 4 replicas off_. Kubernetes uses
_labels_ to match these. Essentially, you make sure your pods are labeled, and
then you indicate in your `Deployment` what pods you're describing. An example
of a label is `app=hi`, another could be `tier=dev`, or `color=blue`. The
latter is obviously no good, `color=yellow` would be better. Labels are
so-called key/value pairs, and you can have multiple different labels on a pod
(but you could not have 2 labels with the same _key_: so `color=blue` and
`color=yellow` would not be work.

On the `Deployment` side, you'd then say I want 3 replicas of all pods matching
_these labels_. For example, you'd indicate that you want all pods with label
`app=hi`:

```
spec:
  selector:
    matchLabels:
      app: hi
```

Easy enough. "But but, we did not give our pod this label, so there'd be
nothing to find". Yes, The final piece of such a `Deployment` would be to see
what kind of pods you want to create, a so-called _pod template_:

```
spec:
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
      - image: stijnh/hi
	name: hi
        ports:
          - containerPort: 8111
```

The latter looks exactly like our pod definition earlier, except that it seems
to also specify that the pod needs to have a label `app=hi` as metadata. Yes,
indeed! In fact, now that have a deployment you can forget all about that
initial pod definition. This `Deployment` knows all it needs to know to create
your 3 pods.  Let's put it all together:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hi-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: hi
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
        - image: stijnh/hi
          name: hi
          ports:
            - containerPort: 8111
```

It describes a `Deployment` with name `hi-deployment`. It specifies that at all
times there need to be 4 replicas of a `Pod` matching `app=hi`. The `template`
indicates how the `Deployment` will go about creating new pods:, it will run
the container `stijnh/hi` and it will label the pod with `app=hi`. The latter
is important as this ensures that the just created Pod would be indeed managed
by the `Deployment`: imagine if we would just a create a `Pod` with label
`app=yo`, the `Deployment` would never think it reached 4 replicas of pods with
label `app=hi` so it'd keep on creating pods.

Before we try this `Deployment`, delete all resources you have so far, and check after that you have 0 pods running:

```
kubectl delete all --all
kubectl get pods
```

Now, create a deployment using the usual `create` based on the above YAML (`hi-deployment.yaml`):

```
kubectl create -f hi-deploymnet.yaml
```

And then watch the magic:

```
$ kubectl get pods
NAME                             READY   STATUS              RESTARTS   AGE
hi-deployment-5f7b895fd9-5hb5h   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-dt8kf   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-r58kc   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-wgwpp   0/1     ContainerCreating   0          3s
```

Note the `STATUS`, and then a couple of seconds later:

```
$ kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
hi-deployment-5f7b895fd9-5hb5h   1/1     Running   0          8s
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          8s
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          8s
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          8s
```

You just specified in 1 lousy file to bring up 4 instances of your app and here
we are. Let's verify that these pods have indeed the label `app=hi` so they're
under management of the `Deployment`:

```
$ kubectl get pods --show-labels
NAME                             READY   STATUS    RESTARTS   AGE     LABELS
hi-deployment-5f7b895fd9-5hb5h   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
```

Yep, there we have it, `app=hi`. We said the Deployment specifies that at all
times there needs to be 4 replicas. So what if we delete one?  For example,
delete the first pod:

```
$ kubectl delete pod hi-deployment-5f7b895fd9-5hb5h
```

Then check your pods, with label `app=hi` (you can use the `-l` flag to specify to only see pods with that label):

```
$ kubectl get pods -l app=hi
NAME                             READY   STATUS    RESTARTS   AGE
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          6m6s
hi-deployment-5f7b895fd9-h7mz7   1/1     Running   0          82s
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          6m6s
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          6m6s
```

Still 4 pods! But if you look at the `NAME`s you'll see that
`hi-deployment-5f7b895fd9-5hb5h` is indeed gone, and replaced by the new pod
`hi-deployment-5f7b895fd9-dt8kf`. The `Deployment` indeed guaranteed that 4
replicas are running at all times. The Zombie Apocalypse is nigh.

You have those 4 pods running, but you can also get more info on the `Deployment` resources running right now as well:

```
$ kubectl get deployment
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
hi-deployment   4/4     4            4           8m48s
```

Note that it indicates that `4` out `4` pods are ready. In this article, I'll
stay at the abstraction of `Deployment` and `Pods` but if you do the following,
you'll see that there's something called a
[`ReplicaSet`](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
created as well:

```
$ kubectl get replicaset
NAME                       DESIRED   CURRENT   READY   AGE
hi-deployment-5f7b895fd9   4         4         4       10m
```

It's actually this `ReplicaSet` that is created by the `Deployment` that makes
sure that the replicas are kept to `4` (see the `DESIRED` column). The
`Deployment` does not create the pods directly. I'll not go into more detail
around `ReplicaSet`s as you'll usually not deal with them directly.



TODO: draw it

TODO: note how we referred to 4 instances of the applciation. we did not talk about number of nodes.


# Conclusion

We did not cover a whole bunch, such as volumes, secrets, jobs, cron jobs, etc etc

# Endnotes

[^sidecar]: We often refer to containers that always run alongside your _main_ container as _sidecars_ for obvious non-inspired reasons.

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-12447521-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'UA-12447521-1');
</script>

<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

