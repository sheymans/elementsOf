<meta charset="utf-8">
                            **Elements of Docker Containers**
                            Stijn Heymans

# Introduction

Docker containers are often presented as magic -- _"don't you dare persisting
anything in a container, it will be lost forever!"_. In this article, I will
try to show that containers are not magic by going through the lifecycle of an
app running on your laptop to an app running in a container on your laptop (the
so-called _containerizing_ of an app).  I will build an image from the app,
publish the image, and run that image in a container. I'll show you the basic
commands of manipulating images, containers, and volumes. 

Some final introductory messages for your general well-being before getting to
the main show:

- I used the book [Docker Deep Dive](https://www.goodreads.com/book/show/39933970-docker-deep-dive) by Nigel Poulton as a reference.
- You can download the demo code I wrote for this article on [github](https://github.com/sheymans/elementsOf/tree/master/docker/demo) to follow along.  The demo is a Javascript app, but there's so little code it should not cause too many side effects when consuming in moderation.  

# The Demo App

There's nothing quite like a toy example that is absolutely nothing like you
will ever encounter in the real world, and I did my utter best to not
disappoint.  I'm going to use the simple web framework
[Express](expressjs.com) for [node](nodejs.org) to write an app that says `Hi`,
clearly a step down from `Hello World`.

This is a simple Javascript file `hi.js`:

~~~js
// Initialize your express app
const express = require('express');
const app = express();

// Tell your app that any GET should be resulting in a `Hi` returned to the user
app.get('/', (request, result) => result.send('Hi'));

// Your app will be listening on port 8111 and it 
// will announce this on the console when you start it
app.listen(8111, () => console.log('hi: listening on port 8111'));
~~~

As I'm using `express`, I'll have a `package.json`[^packagejson] file, that contains `express` as a dependency:

~~~json
{
  "name": "hi",
  "version": "1.0.0",
  "author": "Stijn Heymans",
  "dependencies": {
    "express": "^4.17.1"
  }
}
~~~

To make sure we can run this app, we'll install that `express` dependency as follows:

~~~bash
npm install
~~~

We use the node package manager [npm](https://www.npmjs.com/) to do that. `npm`
will look at the `package.json` file, it will see that there's a `dependency`
`express` and will then install `express` into the directory `node_modules/`.

To start your app, then type:

~~~bash
node hi.js
~~~

You can navigate to http://localhost:8111 to be said `Hi` to. It's a glorious day.

I made these assumptions to be able to run this app on a laptop[^laptop]:

- the laptop allows for installing and running software such as `npm` and `node`.  For example, it's Operating System (OS) is Linux or Mac OS X,
- `npm` was installed, to, in turn, install the required dependencies of your app (`express`); if you did not have it installed, you had to install it
- `npm install` was run from the command-line to install dependencies such as `express`
- `node` was installed to actually run the application; if you did not have it installed, you had to install it
- `node hi.js` was run from the command-line

If you want to install and run your app on another laptop (or, on an [EC2
instance](https://en.wikipedia.org/wiki/Amazon_Elastic_Compute_Cloud) cause
that's how you roll), similar assumptions hold.

_"Well, of course, I would not repeat these steps, I'd put them all in a little
script so I just have to run the script"_. Of course you would. But you'd be
forgetting that a laptop and an EC2 instance are very different things. You'd
probably want scripts per different Operation System at the minimumn.

What if you could package your app while specifying:

- a small Operating System to start from, for example [alpine](https://alpinelinux.org/)
- an installation of `npm` and `node` layered on that
- instructions to install your application (`npm install`)
- instructions to run your application (`node hi.js`)

In other words, what if you could define a small container that specifies the
above and that you run anywhere where such containers can run? (You're getting
the container reference: a standard packaging solution that you can stuff in
there and as long as your stuff fits into a container, you can put it on a ship
and ship it off anywhere in the world).

Next up, let's see how that containerization works.

# Containerizing the App

To containerize your app, we'll use [docker](docker.com). Central to
containerizing an app is the concept of an `image`. There's a certain mysticism
around what an image is, but fundamentally it's an abstract specification that
can be understood by `docker` when it needs to actually run your application.
In that sense, we can compare to several other abstractions in computer
science:

- a _Java class_ specifies the behavior of a particular instance of that class
- a _`jar` file_ packages up your Java application and can then be run by the Java runtime.
- an ISO image for Linux packages up the Operation System so you can download the image, install the image, and run the Operating System
- a Python script specifies which instructions to run for the Python interpreter

All of these comparisons are imperfect, but have in common that there is the
aspect of a specification with all its resources (the `image`) that can then be
run/executed on a machine.

Let's recap the aspects that we'd like to specify for our demo app:

- a small Operating System to start from, for example [alpine](https://alpinelinux.org/)
- an installation of `npm` and `node` layered on that so we can install our app (`npm`) and run our app (`node`)
- instructions to install your application (`npm install`)
- instructions to run your application (`node hi.js`)

Let's write up that specification in a format that `docker` understands and can
build an `image`.  Specifications to build docker images will always be in a
file named `Dockerfile`. Conceptually, they are similar to `package.json`
files: such `package.json` files specify your application's dependencies, a
`Dockerfile` specifies the dependencies your app has from an OS perspective
(what OS does the app assume, what should be installed on the app).

We want our app to be layered on the [Linux alpine OS
system](https://alpinelinux.org/), so the first line of our `Dockerfile` is:

~~~
FROM alpine
~~~

Next, we need an installation of `npm` and `node` layered over that, so let's
specify that in the `Dockerfile`:

~~~
RUN apk add --update nodejs nodejs-npm
~~~

`apk add --update nodejs nodejs-npm` is just the command you'd run on alpine to
install `node` and `npm` (see
[apk](https://wiki.alpinelinux.org/wiki/Alpine_Linux_package_management). If
you picked a different image to start from, you'd might need a different
install command. The important part is `RUN` which runs whatever comes after it
and the installation of `node` and `npm` will constitute a new layer on top of
`alpine`.

Note that the directory where your `Dockerfile` exists is usually called the
_build-context_. We'll then copy all the files from this directory (in our
case, it will copy over `hi.js` and `package.json`) into a root directory
`/src` of the new image we're creating:

~~~
COPY . /src
~~~

The `.` is the current directory (i.e., where the `Dockerfile` lives, the
_build-context_). This `COPY` command will create a new layer in the resulting
image.

At this point, we have an image consisting of a Linux (`alpine`), we've
installed `node` and `npm` into that image and copied our app's source code
into the directory `/src` on the image. Continuing, we use `npm` to install the
`app`.  For this, we'll make the `/src` directory our _current_ working
directory and execute `npm install`:

~~~
WORKDIR /src
RUN npm install
~~~

`WORKDIR` changes the working directory to `/src` and you already know the
`RUN` command, it just executes `npm install` in this case. The latter will
just like it did on your laptop, look for `package.json`, find your
dependencies in there (`express`) and install those.

Almost there. Let's run our application:

~~~
ENTRYPOINT ["node", "./hi.js"]
~~~

This command determines which application to run when we `run` the container. Note that
we did not use `RUN` for this; `RUN` adds a new image layer, whereas
`ENTRYPOINT` just adds metadata and appears only once in the `Dockerfile` to
indicate _this is what you run once the container is ready to be run_.

We forgot one thing, which is that our app will be listening on port `8111`.
Our app would run entirely in the container so we'd not be able to access it
via `localhost` on our laptop publish the container's port. The `EXPOSE`
statement is a _soft_ indication that this app will be listening on port `8111`
(it does not actually yet allow you to access that port from your laptop, see
later, as such this is merely a _heads-up_ for the engineer reading the
`Dockerfile`).

~~~
EXPOSE 8111
~~~

To round it off, we add some more metadata to the image:

~~~
LABEL maintainer="ben@ny.com"
~~~

This is the whole `Dockerfile`:

~~~
FROM alpine
RUN apk add --update nodejs nodejs-npm
COPY . /src
WORKDIR /src
RUN npm install
ENTRYPOINT ["node", "./hi.js"]
EXPOSE 8111
LABEL maintainer="ben@ny.com"
~~~

We can then use this `Dockerfile` to build an image:

~~~
 docker image build -t hi:latest .
~~~

Dissected:

- `docker`: the `docker` program that we use to build images, run images, etc
- `image`: tell `docker` that we're doing involves images (you'd also have `docker container` to indicate that you're working with containers)
- `build`: build an image
- `-t hi:latest`: we're *t*agging an image with name `hi` and with tag `latest`. A tag can be used to identified different versions of your software (`v1`, etc) 
- `.`: don't forget the `.`. The `.` stands for _look into your current directory for the Dockerfile_. Note that `-t` indeed stands for _tag_ and not _target_ (there's a `--target` option)

Let's have a look at whether we now indeed have an image available:

~~~
docker image ls
~~~

Dissecting that command:

- `docker`: the `docker` program (no, really?)
- `image`: we're dealing with images (what's the images around?)
- `ls`: the usual listing command

This should give as output something like:

~~~
REPOSITORY     TAG       IMAGE ID            CREATED             SIZE
hi             latest    5d3095b79194        5 minutes ago       57.2MB
~~~

This shows our created image with repository name `hi`, tag `latest`, and
image id `5d3095b79194` -- the image id will be different on your machine.

Sometimes you'd just want the `IMAGE ID`s,for example, to pipe
them to another command. For that situation, you can use `--format`. That option uses [Go
templates](https://golang.org/pkg/text/template/) to extract the data you like
to see formatted. For example, let's say we want to see the size:

~~~
docker image ls --format "{{.Size}}"
~~~

This will show:

~~~
57.2MB
~~~

More useful is often getting the Image Ids, so you try:

~~~
docker image ls --format "{{.ImageId}}"
~~~

This unfortunately gives you the error:

~~~
Template parsing error: template: :1:2: executing "" at <.ImageId>: can't evaluate field ImageId in type formatter.imageContext
~~~

So while `docker image ls` shows `IMAGE ID` as a column name, that's not how
you can access this. There's a nice
[hint](https://docs.docker.com/config/formatting/) on how you can see the
attributes that you can actually use in `--format`:

~~~
docker image ls --format "{{json .}}"
~~~

This will make a json out of _everything_ and will show:

~~~
{"Containers":"N/A","CreatedAt":"2020-03-25 08:11:44 -0700 PDT","CreatedSince":"2 days ago","Digest":"\u003cnone\u003e","ID":"5d3095b79194","Repository":"hi","SharedSize":"N/A","Size":"57.2MB","Tag":"latest","UniqueSize":"N/A","VirtualSize":"57.23MB"}
~~~

From which you can see that the `IMAGE ID` is actually under attribute `ID`:

~~~
docker image ls --format "{{.ID}}"
~~~

will give you `5d3095b79194`.

Finally, have a closer look at our created image:

~~~
docker image inspect 5d3095b79194
~~~

You see your label `maintainer` added, the entrypoint `node ./hi.js`, and the
different layers with different ids that were created as part of this image
(recall that each `RUN` created a different layer). Grab a cup of coffee and
spend some time quality time with that `docker image inspect` output, cause
you're worth it.

# Publishing the Image

In this article, I'll keep working with the local image just created. However,
if you want other users to be able to use your image, you'd upload it, for
example to the [Docker Hub](https://hub.docker.com/). After you uploaded your
image to a Docker Hub repository, other developers can then _pull_ it. Have a
look at this
[tutorial](https://ropenscilabs.github.io/r-docker-tutorial/04-Dockerhub.html)
if you'd like to see some of the steps involved, cause that's all I'll say
about publishing your image.

# Running the Container

Now that I have an image `hi:latest`, I can create a running instance of that
image. Where before we used `docker image` to do image manipulation, we'll now
use `docker container` indicating that we're running containers based off an
image:

~~~
docker container run -d hi:latest
~~~

This basic command starts the container in detached/background mode (hence the
`-d`) and uses the image `hi:latest` to instantiate the container with. Since
we specified that this container should be running `node ./hi.js` using its
`ENTRYPOINT` statement in the `Dockerfile`, we expect to see this app running
when we _jump into_ the container.

Before getting into detail of the prosaic _jumping into_ containers, I need to
know what the ID or name of the freshly started container is:

~~~
docker container ls
~~~

We'd see something like this as output:

~~~
CONTAINER ID      IMAGE      COMMAND         CREATED        STATUS        PORTS     NAMES
885288159d28      hi:latest  "node ./hi.js"  2 minutes ago  Up 2 minutes  8111/tcp  boring_dijkstra
~~~

The running container has ID `885288159d28` and name `boring_dijkstra`.  We did
not specify a name when running the container, so we got a random
`boring_dijkstra`. However, you can define your own container name using:
`docker container run --name exciting_dijkstra -d hi:latest`.

We see from that `docker container ls` that there's a container running with id
`885288159d28` and it's executing the command `node ./hi.js` which is exactly
what we expected from the `Dockerfile`.

With a name for our runnning container, we can do some _jumping
into_:

~~~
docker container exec -it boring_dijkstra sh
~~~

!!! Tip
    I used the the container's name, `boring_dijkstra`, but I can also use its ID `885288159d28`

After executing that command, you see a shell prompt appear ( maybe you'll even
see a  &#128026;[^shell] ) and if you check what processes are running in this
container using <a href="https://en.wikipedia.org/wiki/Ps_(Unix)">`ps`</a>, you'll indeed
see our `node` app running as expected/hoped for:

~~~
/src # ps
PID   USER     TIME  COMMAND
  1   root     0:00  node ./hi.js
 32   root     0:00  sh
 37   root     0:00  ps
~~~

You'll also see `sh` which is the command we executed as part of `docker container
exec -it boring_dijkstra sh` and of course `ps` which is the command we just
ran in that shell.

Let's analyze the command `docker container exec -it boring_dijkstra sh`:

- `docker container`: you're executing docker and it involves containers (in contrast to `image` or `volume` see later)
- `exec`: you're going to execute a command in the container, which command? `sh`.
- `boring_dijkstra`: the container you're executing the command `sh` in
- `-it`: `-t` is short for `--tty` and attaches you to a terminal in the container. `-i` is to make input and output from the container flow to your terminal on your docker host (your laptop). You almost always see `-it` being used together: try out `-t` on its own in the previous command and try pressing `return`; you'll see nothing happen as the output of the container is not flowing to the output of your laptop terminal. I find it most useful to think of `-it` as `interactive mode` and `-d` as detached mode (the container runs but you're not _in it_).
- `sh`: the command you're executing on your container.

The documentation on
[exec](https://docs.docker.com/engine/reference/commandline/exec/) as well as
this [stackoverflow
article](https://stackoverflow.com/questions/22272401/what-does-it-mean-to-attach-a-tty-std-in-out-to-dockers-or-lxc)
have more details that are worth reading through.

By _jumping into_ the container, I confirmed the app is running. As I wrote
this app, I know that it runs on port `8111`.  Indeed in `hi.js`:

~~~
app.listen(8111, () => console.log('hi: listening on port 8111'));
~~~

In the `Dockerfile`, I also exposed that port:

~~~
EXPOSE 8111
~~~

In contrast to what the big `EXPOSE` hints at, this does not actually make the
port `8111` _reachable_; you can see this more as documentation for other
engineers reading your `Dockerfile` that your service listens to this port
(`8111`).

To actually make the port `8111` that the app listens to reachable from outside
your container, say from your laptop, you publish it with the `-p` option.

Before using that `-p` option, let's first `stop` and `rm` our currenty running container:

~~~
docker container stop boring_dijkstra
docker container rm boring_dijkstra
~~~

And let's now start a container from the `hi:latest` image where we map the
Docker host's port `8000` (your laptop) to `8111` port. For
reading-left-to-right oriented cultures, this is roughly from outward to inward
(left to right).

~~~
docker container run -d -p 8000:8111 hi:latest
~~~

After this you can navigate to `localhost:8000` on your browser and see `Hi`.
Note that going to `localhost:8111` in your browser will give an error as
`8111` is exposed in your container, not your `localhost` (laptop).

Before diving into persistence, let's look into these `stop` and `rm`
commands we executed before starting a new container.

What's the difference between `stop` and `rm`? If you remove (`rm`) a
container, you can no longer start it or refer to it using its name. Neither
will a `docker container ls -a` show your container.  Anything your application
wrote (for example, to `/tmp`, any logs, anything) is gone. It's gone.

If you `stop` a container, you merely suspend it. After stopping a container,
you can restart it with `docker container start your_container_name`. After
starting it again, whatever you wrote to `/tmp` before stopping it is still
there. Your ap will not be running if the container is stopped. For example, if
you stop your container with `docker container stop your_container_name`,
navigating to `localhost:8000` will give an error.  Restarting the container on
the other hand, brings `localhost:8000` back to life.

# Persistence

Mention _persistence_ in the context of docker or
[kubernetes](https://kubernetes.io/) and everyone gets upset: _there's no
persistence in containers! you dumb?!_. While there's enjoyment in upset
people, overall it's to be avoided due to the negative impact on one's own
well-being.

Of course you can persist stuff from a container. What do I mean with persist?
A typical example would be logs. You can write logs and not lose them
after the container is removed. There are other types of persistence like
writing to a database table (and yes, of course, your data and table would
still exist outside of your container, presuming that it lives _outside_ your
container, or in other words, has a lifecycle that is independent of your
container). Your pet rat is not going to die when your container dies. Their
lifes are not tied together (unless you keep your pet rat in a container; you
should really not be keeping pet rats in general cause pet rats are really just
rats and keeping anything alive in a container that is not your lunch is a
no-no).

_But I'm talking about writing to the file-system man!! you can't do that man!! you're not
listening!!_. Your skills of observation are something I will write home about
my friend. I do not listen.

The _magic_ that allows docker to write to _the_ (a?) file-system that persist
after the container is long gone and forgotten is called _volumes_.

Before explaining volumes, I want to keep insisting on persisting. What type of
persistance to a file-system do we already have without volumes?  I can have my
app that runs in a container write to `/tmp` without issues while the container
is running. I can even `stop` the container (suspending it, as we saw above)
and when I come back to this container, the data I wrote to `/tmp` would still
be there. It's only when I `rm` the container that data in `/tmp` would be gone
as `rm` is literally wiping away the file-system of the container.

That's less drama than you were hoping for. Let's bring that disappointment in
life up another notch.  The key to volumes is that they live and die outside of
your container's life cycle. They're an abstraction of storage: I can write to
a volume, I can read from a volume, so my logs, yes, they can go there. So if I
have these 2 independent entities: a volume that I can write to/read from and a
container that potentially could be doing that writing and reading, how do I
connect those? Well, you mount the volume to a directory on your container. So
if your container has a directory `/logs`, you can then mount your volume to
the directory `/logs`. After that mounting, anything written to the `/logs`
directory will be written to mounted volume, everything read from `/logs` will
actually be read from the mounted volume.

Let's create a volume:

~~~
docker volume create dramatic_vol
~~~

Similar to the above, where we had `docker image` for image manipulation,
`docker container` for container manipulation, we have `docker volume` for
volume manipulation. Check whether your freshly created volume is indeed there with `ls`:

~~~
docker volume ls
~~~

Let's now attach `dramatic_vol` to a container running based on the image `hi:latest`:

~~~
docker container run -d --mount target=/src,source=dramatic_vol hi:latest
~~~

Let's find out the name of that container:

~~~
$ docker container ls
CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS              PORTS               NAMES
0abd6c82cee2        hi:latest           "node ./hi.js"      54 seconds ago      Up 52 seconds       8111/tcp            laughing_pike
~~~

And let's then check out the container `laughing_pike`:

~~~
docker container inspect laughing_pike
~~~

You'll see lots of output, among which this snippet:

~~~
 "Mounts": [
            {
                "Type": "volume",
                "Name": "dramatic_vol",
                "Source": "/var/lib/docker/volumes/dramatic_vol/_data",
                "Destination": "/src",
                "Driver": "local",
                "Mode": "z",
                "RW": true,
                "Propagation": ""
            }
        ],
~~~

So there you go: our volume `dramatic_vol` is part of the mounts, it indicates
where the data for that volume actually lives on my laptop
(`/var/lib/docker/volumes/dramatic_vol/_data`) and it indicates to what
directory _in the container_ it was  tied: `/src`.  Recall that anything
written to `/src` by my app in the container would be written to the volume
(and you could check that out at `/var/lib/docker/volumes/dramatic_vol/_data`).

If you `stop` and `rm` the container, none of the data on the volume would be
affected. It is persistent.

# Conclusion

I showed you how to build an image out of your app (_containerize_ it) , how to
run a container based on that image, and how to use volumes to persist the data
that you write in the container beyond the life cycle of that container.
There's a lot I didn't talk about as I think this is enough basics to allow you
to delve further. Things I know I left out: discussions on tags, docker swarm,
docker networking, `docker-compose`. Things I don't know I left out: $\infty$.

# Endnotes

[^laptop] I will mostly refer to your _laptop_ but you could be working on a desktop computer, or be logged in to an EC2 instance.

[^packagejson] A `package.json` is used to describe your application (the
metadata for your example) so that `npm` (which can be used to build your
application) knows _how_ to build it. You can read some more about it
[here](https://nodejs.org/en/knowledge/getting-started/npm/what-is-the-file-package-json/).

[^shell] Of course [not](https://en.wikipedia.org/wiki/Unix_shell), but I admire your outlook on life.

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-12447521-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'UA-12447521-1');
</script>

<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

