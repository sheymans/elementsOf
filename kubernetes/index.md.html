<meta charset="utf-8">
                            **Elements of Kubernetes**
                            Stijn Heymans
                            published: N/A

# Introduction

I'll go over what I think are the elements of a kubernetes _from an application
developer's perspective_. A certain familiarity with deploying applications in
a production environment is expected (if you can imagine what things like
_downtime_, _rollback_, _production_ mean, you should be OK).  I'll do this by
assuming you have an application and there are a couple of requirements you
have on this application before you will be able to put it in production:

- You need to run many instances of your application to be able to serve traffic you have, which runs into the 1000s of request per second,
- Clients of your application will want to connect to one address, requests will be guided to different instances of your application automatically
- You need to be able to push changes to your application without downtime
- You need to be able to rollback changes to your application
- There is basic warm-up that needs to happen with your application before it can start serving up requests (for example, it needs to sync its data with data other instances of the application have available)
- Sometimes an instance of your application will die and stop serving requests, you have not figured out why, but you want that instance of the application to become invisible to clients of your application. Traffic should be guided to still functioning instances of your application
- To save costs during the night when you need less capability on your application, you'll want to reuse some of your machines that the instances of application normally run on to run batch jobs for maintenance
- You will have regular events (oh, say shopping events) during which the amount of machines to serve your traffic will not be enough, so you'll want to scale up the amount of machines you have available for your cluster. After the event, you'll want to get back to normal and scale down,

In comes Kubernetes (we will refer to _Kubernetes_ by its common abbreviation
of _k8s_ -- pronounced _kates_). While Kubernetes is massive hammer, it does
hammer the above problems squarely away. I dispariginaly say _hammer_ as it
indeed is able to do more than what you'd need. Its historic origin play a role
in that as used by a Cloud Provider in which many different applications run on
the same host of machines and they'd want to optimize usage of infrastructure.
In the use case we laid out, we are not _that_ interested in running different
applications on the same infrastructure, although the particular requirement to
save cost during the night and run some batch jobs on the same machine that
your machine runs on during the day, is designed to hint at that use.

The requirements as laid out above come from my personal experience, and the
most knowledge I have on k8s comes from [Kubernetes in
Action](https://www.goodreads.com/book/show/34013922-kubernetes-in-action) by
Marko Luk≈°a. The book goes in tremendous depth and is useful for infrastructure
administrators (which is a whole world on itself that I will barely touch on
here). I'll stick to what I know which is application development and what I
need to run an application in production satisfying the above requirements.

# Your Application and Running Many Instances of It

## A Containerized Application

For the application, I'll be running a simple web server that serves up the
sentence `Hi from HOST` where `HOST` will be the host it is running from. We're
in a brave new world, so this application has been packed up such that it can
run as a Docker container.

I've posted the source of this application on
[github](https://github.com/sheymans/elementsOf/tree/master/kubernetes/demo/hi_app)
and I've pushed the container image to the [Docker
Hub](https://hub.docker.com/) with name
[stijnh/hi_app](https://hub.docker.com/repository/docker/stijnh/hi).

If at this point, you feel slightly lost and it's related to this article, this
may be a good time to take a break from this article and read up on the
[Elements of Docker
Containers](http://www.stijnheymans.net/elements_of_docker_containers.html).

The container image is public so you can try out the app on your laptop right now with:

```
docker container run -d -p 8111:8111 -t stijnh/hi
```

Recall that `docker container run` indicates you're asking to run a container
image; `-d` indicates you're going to do that in the background as a daemon
(rather than interactive), `-p 8111:8111` means you're mapping your `localhost`
port 8111 (on your laptop) to port `8111` on your docker container behind which
your app is running, `-t stijnh/hi` indicates the image you're using to get
the container from.

At this point you can open your browser, and navigating to `localhost:8111`
should show you `Hi from fbd698569ec5` where `fbd698569ec5` will be different
of course for you as this the container's hostname.

Alright, at this point, I'll refer to this state as "you have 1 app
instance running". It's running as a container on your laptop. Your laptop in
this sense is the _machine_ (or if you feel like a rockstar, you're _bare metal
machine_).

## Running Many Instances of It

### Pod

We have 1 instance of our app running, but now you'll have thousands of users
saying hi, so you can't have only 1 instance -- that will not fit your
megalomanic dreams!  We need more! We need at least 3!

But first we need to define a bit better of what we need 3. We clearly need 3
containers (each container runs our app). As this is example is illustrating a
couple of elements of kubernetes, we've not mentioned real-world things like
logging. Typically, your application will be writing logs, and you'll need
something to _rotate_ these logs away to more permanent storage (to [AWS's S3](https://aws.amazon.com/s3/)
for example, where you could query it using
[Athena](https://aws.amazon.com/athena/)). We'd want this log writing in a
separate container (other apps could use it as well so it's not typical for
this app alone). You'd want these 2 containers always running
together[^sidecar], so if I say I want 3 instances of my app, I actually want 3
instances of my app with the logging container. Hence, the first level of
abstraction -- the base unit it deals with -- is _not_ a container, it's
something called a _pod_ in kubernetes. So let's do some more wrapping, and
wrap that container into a _Pod_ manifest[^manifest] that describes what the
pod should look like. If you recall `Dockerfile`s, then this is similar in
philosophy: in a `Dockerfile` you describe your app so that `docker` knows how
to build an image, in a _Pod_ manifest you describe what all goes in your
_Pod_, most importantly what container image to use such k8s knows how to
create a Pod.

Pods are usually defined in [YAML](https://en.wikipedia.org/wiki/YAML) files,
so let's do this. I have a file `hi-pod.yaml`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hi-pod
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

which is really doing nothing more than naming my pod `hi-pod` and indicating
that it's running the container
[`stijnh/hi`](https://hub.docker.com/repository/docker/stijnh/hi).

Let's go over the pod definition, line by line:

```
apiVersion: v1
```

Indicates what API version of kubernetes this kubernetes _resource_ is defined
in (_Pods_, together with a whole lot of other things, are named _resources_ in
k8s), in this case `v1`. I have no memory, nor patience for remembering API
versions, but there are [explanations of what version to use when out
there](https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html).



```
kind: Pod
```

The type of resource you're defining, a `Pod`.

```
metadata:
  name: hi-pod
```

Some metadata about the pod, in this case its name `hi-pod`.

```
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

The pod's specification, in this case a list of 1 container, where the
container is described by it's image tag `stijnh/hi`, given a name `hi`, and
indicating that the container exposes the port `8111`.

If you only remember one thing from the above, may it be that you'll see 4
parts of such a resource description which you'll see with other types of
resources as well: the `apiVersion`, the `kind`, the `metadata`, and the
`spec`.

With that said, can we deploy this pod now please? Yes. I'm going to assume
you're on your laptop and have something like
[minikube](https://github.com/kubernetes/minikube) installed. As I'm tackling
k8s from the perspective of an application developer, I'll not get into the
details of setting up a cluster that serves production traffic. 

This _is_ a good time to to talk about some general k8s concepts. But first, go
check out [minikube](https://github.com/kubernetes/minikube) and install it so
you can try out some stuff on your laptop.

Once you're back (you were never gone though, were you), you might have read
that minikube will give you a _single-node cluster_. You'll hear _cluster_ a
lot, so what is a cluster? My working definition is that it's a set of nodes.
What's a _node_ then? Again, my working definition is _that's a machine, an EC2
instance, a laptop, some kind of computor with a CPU and memory, an old Pentium
tower in a dusty basement, ..._ you get the idea. A typical mapping in my head
would be _if I have 40 EC2 instances in the EU region to serve traffic_, that's
a _cluster_ of _40_ nodes. 


Now that you have `minikube` installed, this command should show something:

```shell
$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   0d   v1.18.3
```

Your first use of `kubectl`, but not your last. `kubectl` is the command-line
tool that will allow you to interact with your cluster asking it about its
nodes, its pods, ...In this case, we know that we 1 node running, called
`minikube`. This is good to keep in mind, all example we'll show later will
involve 1 node. In reality, you'll be working with clusters that have many many
more nodes of course.

Alright, we have a cluster with 1 node. We have our `hi` app, tucked away in a
container, and we defined a pod that is supposed to run that container. Let's
deploy that pod using `kubectl`:

```shell
$ kubectl create -f hi_pod.yaml
pod/hi-pod created
```

Verify that your pod is created:

```shell
$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
hi-pod   1/1     Running   0          54s
```

This shows us that the pod is `Running` and has been doing so for `54s`. Note the levels of indirection: 

- when we ran the app `hi.js` we could just navigate to `localhost:8111` and see the `Hi from...`
- when we ran the app in a container, we needed to forward the localhost's port `8111` _into_ the container using `docker container run -d -p 8111:8111 -t stijnh/hi`
- when we run the app in a container in a pod, we need to do what?

We could forward ports via `kubectl` as well, but that would be mostly be for
debugging. We'll defer to answering this question to the next section where
we'll talk about how to expose your pods to other pods. For now, I'll just show how you can check the logs of that pod:

```
$ kubectl logs hi-pod
hi: listening on port 8111 of host hi-pod
```

That's indeed what the app logs. Note that the host is listed as `hi-pod` which
corresponds to the name of the pod.


You can more details on your pod by doing a `describe`:

```shell
$ kubectl describe pod hi-pod
```

There will be a lot of output, but interesting is for example the last section:

```shell
Events:
  Type    Reason     Age        From               Message
  ----    ------     ----       ----               -------
  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned default/hi-pod to minikube
  Normal  Pulling    23h        kubelet, minikube  Pulling image "stijnh/hi"
  Normal  Pulled     23h        kubelet, minikube  Successfully pulled image "stijnh/hi"
  Normal  Created    23h        kubelet, minikube  Created container hi
  Normal  Started    23h        kubelet, minikube  Started container hi
```

You'll see the container image `stijnh/hi` being pulled and then the container being created. Summarizing this with a picture, we have our container image, deployed in a pod on 1 node:

![Running 1 pod on your laptop](./diagrams/pod.svg)


### Deployment

TODO: show how to get multiple pods

# Conclusion

We did not cover a whole bunch, such as volumes, secrets, jobs, cron jobs, etc etc

# Endnotes

[^sidecar]: We often refer to containers that always run alongside your _main_ container as _sidecars_ for obvious non-inspired reasons.

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-12447521-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'UA-12447521-1');
</script>

<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

