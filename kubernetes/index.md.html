<meta charset="utf-8">
                            **Elements of Kubernetes**
                            Stijn Heymans
                            published: N/A

# Introduction

I'll go over what I think are the elements of a kubernetes _from an application
developer's perspective_. A certain familiarity with deploying applications in
a production environment is expected (if you can imagine what things like
_downtime_, _rollback_, _production_ mean, you should be OK).  I'll do this by
assuming you have an application and there are a couple of requirements you
have on this application before you will be able to put it in production:

- You need to run many instances of your application to be able to serve traffic you have, which runs into the 1000s of request per second,
- Clients of your application will want to connect to one address, requests will be guided to different instances of your application automatically
- You need to be able to push changes to your application without downtime
- You need to be able to rollback changes to your application
- There is basic warm-up that needs to happen with your application before it can start serving up requests (for example, it needs to sync its data with data other instances of the application have available)
- Sometimes an instance of your application will die and stop serving requests, you have not figured out why, but you want that instance of the application to become invisible to clients of your application. Traffic should be guided to still functioning instances of your application
- To save costs during the night when you need less capability on your application, you'll want to reuse some of your machines that the instances of application normally run on to run batch jobs for maintenance
- You will have regular events (oh, say shopping events) during which the amount of machines to serve your traffic will not be enough, so you'll want to scale up the amount of machines you have available for your cluster. After the event, you'll want to get back to normal and scale down,

In comes Kubernetes (we will refer to _Kubernetes_ by its common abbreviation
of _k8s_ -- pronounced _kates_). While Kubernetes is massive hammer, it does
hammer the above problems squarely away. I dispariginaly say _hammer_ as it
indeed is able to do more than what you'd need. Its historic origin play a role
in that as used by a Cloud Provider in which many different applications run on
the same host of machines and they'd want to optimize usage of infrastructure.
In the use case we laid out, we are not _that_ interested in running different
applications on the same infrastructure, although the particular requirement to
save cost during the night and run some batch jobs on the same machine that
your machine runs on during the day, is designed to hint at that use.

The requirements as laid out above come from my personal experience, and the
most knowledge I have on k8s comes from [Kubernetes in
Action](https://www.goodreads.com/book/show/34013922-kubernetes-in-action) by
Marko Luk≈°a. The book goes in tremendous depth and is useful for infrastructure
administrators (which is a whole world on itself that I will barely touch on
here). I'll stick to what I know which is application development and what I
need to run an application in production satisfying the above requirements.

# Your Application and Running Many Instances of It

## A Containerized Application

For the application, I'll be running a simple web server that serves up the
sentence `Hi from HOST` where `HOST` will be the host it is running from. We're
in a brave new world, so this application has been packed up such that it can
run as a Docker container.

I've posted the source of this application on
[github](https://github.com/sheymans/elementsOf/tree/master/kubernetes/demo/hi_app)
and I've pushed the container image to the [Docker
Hub](https://hub.docker.com/) with name
[stijnh/hi_app](https://hub.docker.com/repository/docker/stijnh/hi).

If at this point, you feel slightly lost and it's related to this article, this
may be a good time to take a break from this article and read up on the
[Elements of Docker
Containers](http://www.stijnheymans.net/elements_of_docker_containers.html).

The container image is public so you can try out the app on your laptop right now with:

```
docker container run -d -p 8111:8111 -t stijnh/hi
```

Recall that `docker container run` indicates you're asking to run a container
image; `-d` indicates you're going to do that in the background as a daemon
(rather than interactive), `-p 8111:8111` means you're mapping your `localhost`
port 8111 (on your laptop) to port `8111` on your docker container behind which
your app is running, `-t stijnh/hi` indicates the image you're using to get
the container from.

At this point you can open your browser, and navigating to `localhost:8111`
should show you `Hi from fbd698569ec5` where `fbd698569ec5` will be different
of course for you as this the container's hostname.

Alright, at this point, I'll refer to this state as "you have 1 app
instance running". It's running as a container on your laptop. Your laptop in
this sense is the _machine_ (or if you feel like a rockstar, you're _bare metal
machine_).

## Running Many Instances of It

### Pod

We have 1 instance of our app running, but now you'll have thousands of users
saying hi, so you can't have only 1 instance -- that will not fit your
megalomanic dreams!  We need more! We need at least 3!

But first we need to define a bit better of what we need 3. We clearly need 3
containers (each container runs our app). As this is example is illustrating a
couple of elements of kubernetes, we've not mentioned real-world things like
logging. Typically, your application will be writing logs, and you'll need
something to _rotate_ these logs away to more permanent storage (to [AWS's S3](https://aws.amazon.com/s3/)
for example, where you could query it using
[Athena](https://aws.amazon.com/athena/)). We'd want this log writing in a
separate container (other apps could use it as well so it's not typical for
this app alone). You'd want these 2 containers always running
together[^sidecar], so if I say I want 3 instances of my app, I actually want 3
instances of my app with the logging container. Hence, the first level of
abstraction -- the base unit it deals with -- is _not_ a container, it's
something called a _pod_ in kubernetes. So let's do some more wrapping, and
wrap that container into a _Pod_ manifest[^manifest] that describes what the
pod should look like. If you recall `Dockerfile`s, then this is similar in
philosophy: in a `Dockerfile` you describe your app so that `docker` knows how
to build an image, in a _Pod_ manifest you describe what all goes in your
_Pod_, most importantly what container image to use such k8s knows how to
create a Pod.

Pods are usually defined in [YAML](https://en.wikipedia.org/wiki/YAML) files,
so let's do this. I have a file `hi-pod.yaml`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hi-pod
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

which is really doing nothing more than naming my pod `hi-pod` and indicating
that it's running the container
[`stijnh/hi`](https://hub.docker.com/repository/docker/stijnh/hi).

Let's go over the pod definition, line by line:

```
apiVersion: v1
```

Indicates what API version of kubernetes this kubernetes _resource_ is defined
in (_Pods_, together with a whole lot of other things, are named _resources_ in
k8s), in this case `v1`. I have no memory, nor patience for remembering API
versions, but there are [explanations of what version to use when out
there](https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html).



```
kind: Pod
```

The type of resource you're defining, a `Pod`.

```
metadata:
  name: hi-pod
```

Some metadata about the pod, in this case its name `hi-pod`.

```
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

The pod's specification, in this case a list of 1 container, where the
container is described by it's image tag `stijnh/hi`, given a name `hi`, and
indicating that the container exposes the port `8111`.

If you only remember one thing from the above, may it be that you'll see 4
parts of such a resource description which you'll see with other types of
resources as well: the `apiVersion`, the `kind`, the `metadata`, and the
`spec`.

With that said, can we deploy this pod now please? Yes. I'm going to assume
you're on your laptop and have something like
[minikube](https://github.com/kubernetes/minikube) installed. As I'm tackling
k8s from the perspective of an application developer, I'll not get into the
details of setting up a cluster that serves production traffic. 

This _is_ a good time to to talk about some general k8s concepts. But first, go
check out [minikube](https://github.com/kubernetes/minikube) and install it so
you can try out some stuff on your laptop.

Once you're back (you were never gone though, were you), you might have read
that minikube will give you a _single-node cluster_. You'll hear _cluster_ a
lot, so what is a cluster? My working definition is that it's a set of nodes.
What's a _node_ then? Again, my working definition is _that's a machine, an EC2
instance, a laptop, some kind of computor with a CPU and memory, an old Pentium
tower in a dusty basement, ..._ you get the idea. A typical mapping in my head
would be _if I have 40 EC2 instances in the EU region to serve traffic_, that's
a _cluster_ of _40_ nodes. 


Now that you have `minikube` installed, this command should show something:

```shell
$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   0d   v1.18.3
```

Your first use of `kubectl`, but not your last. `kubectl` is the command-line
tool that will allow you to interact with your cluster asking it about its
nodes, its pods, ...In this case, we know that we 1 node running, called
`minikube`. This is good to keep in mind, all example we'll show later will
involve 1 node. In reality, you'll be working with clusters that have many many
more nodes of course.

Alright, we have a cluster with 1 node. We have our `hi` app, tucked away in a
container, and we defined a pod that is supposed to run that container. Let's
deploy that pod using `kubectl`:

```shell
$ kubectl create -f hi_pod.yaml
pod/hi-pod created
```

Verify that your pod is created:

```shell
$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
hi-pod   1/1     Running   0          54s
```

This shows us that the pod is `Running` and has been doing so for `54s`. Note the levels of indirection: 

- when we ran the app `hi.js` we could just navigate to `localhost:8111` and see the `Hi from...`
- when we ran the app in a container, we needed to forward the localhost's port `8111` _into_ the container using `docker container run -d -p 8111:8111 -t stijnh/hi`
- when we run the app in a container in a pod, we need to do what?

We could forward ports via `kubectl` as well, but that would be mostly be for
debugging. We'll defer to answering this question to the next section where
we'll talk about how to expose your pods to other pods. For now, I'll just show how you can check the logs of that pod:

```
$ kubectl logs hi-pod
hi: listening on port 8111 of host hi-pod
```

That's indeed what the app logs. Note that the host is listed as `hi-pod` which
corresponds to the name of the pod.


You can more details on your pod by doing a `describe`:

```shell
$ kubectl describe pod hi-pod
```

There will be a lot of output, but interesting is for example the last section:

```shell
Events:
  Type    Reason     Age        From               Message
  ----    ------     ----       ----               -------
  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned default/hi-pod to minikube
  Normal  Pulling    23h        kubelet, minikube  Pulling image "stijnh/hi"
  Normal  Pulled     23h        kubelet, minikube  Successfully pulled image "stijnh/hi"
  Normal  Created    23h        kubelet, minikube  Created container hi
  Normal  Started    23h        kubelet, minikube  Started container hi
```

You'll see the container image `stijnh/hi` being pulled and then the container being created. Summarizing this with a picture, we have our container image, deployed in a pod on 1 node:

![Running 1 pod on your laptop](./diagrams/pod.svg)


### Deployment

Now that we have 1 instance of your application running (on 1 pod, in 1
container). We indicated as a requirement that _your application to be able to
serve traffic you have, which runs into the 1000s of request per second_ and
turns out 1 instance of your application is not able to do that. Assume each
application can serve 250 requests per second, and we need to be able to serve
1000 in total. Hence, we need 4 instances of the application running.

''OK, let me run 4 containers of my application'', you say. "yes, but no" I
reply. The unit of operation when your application is managed by k8s is a pod,
so we'll want 4 pods (each in turn running 1 container).

There's a kunbernetes resource type (the first one we met was a `Pod`) that
allows you to specify how many pods k8s should be keeping around: a
[`Deployment`](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). 

So that will be the core functionality of a `Deployment` that I'm interested
in at this point, specify how many of a certain pod I'd like around. As with
pods, deployments are specified using YAML manifests and the line that will get
me my 4 pods is:

```
spec:
  replicas: 3
```

Glad to see you're awake Grashopper, yeah 4 of course:

```
replicas: 4
```

How will you describe of _which pod you want 4 replicas off_. Kubernetes uses
_labels_ to match these. Essentially, you make sure your pods are labeled, and
then you indicate in your `Deployment` what pods you're describing. An example
of a label is `app=hi`, another could be `tier=dev`, or `color=blue`. The
latter is obviously no good, `color=yellow` would be better. Labels are
so-called key/value pairs, and you can have multiple different labels on a pod
(but you could not have 2 labels with the same _key_: so `color=blue` and
`color=yellow` would not be work.

On the `Deployment` side, you'd then say I want 3 replicas of all pods matching
_these labels_. For example, you'd indicate that you want all pods with label
`app=hi`:

```
spec:
  selector:
    matchLabels:
      app: hi
```

Easy enough. "But but, we did not give our pod this label, so there'd be
nothing to find". Yes, The final piece of such a `Deployment` would be to see
what kind of pods you want to create, a so-called _pod template_:

```
spec:
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
      - image: stijnh/hi
	name: hi
        ports:
          - containerPort: 8111
```

The latter looks exactly like our pod definition earlier, except that it seems
to also specify that the pod needs to have a label `app=hi` as metadata. Yes,
indeed! In fact, now that have a deployment you can forget all about that
initial pod definition. This `Deployment` knows all it needs to know to create
your 3 pods.  Let's put it all together:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hi-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: hi
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
        - image: stijnh/hi
          name: hi
          ports:
            - containerPort: 8111
```

It describes a `Deployment` with name `hi-deployment`. It specifies that at all
times there need to be 4 replicas of a `Pod` matching `app=hi`. The `template`
indicates how the `Deployment` will go about creating new pods:, it will run
the container `stijnh/hi` and it will label the pod with `app=hi`. The latter
is important as this ensures that the just created Pod would be indeed managed
by the `Deployment`: imagine if we would just a create a `Pod` with label
`app=yo`, the `Deployment` would never think it reached 4 replicas of pods with
label `app=hi` so it'd keep on creating pods.

Before we try this `Deployment`, delete all resources you have so far, and check after that you have 0 pods running:

```
kubectl delete all --all
kubectl get pods
```

Now, create a deployment using the usual `create` based on the above YAML (`hi-deployment.yaml`):

```
kubectl create -f hi-deploymnet.yaml
```

And then watch the magic:

```
$ kubectl get pods
NAME                             READY   STATUS              RESTARTS   AGE
hi-deployment-5f7b895fd9-5hb5h   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-dt8kf   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-r58kc   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-wgwpp   0/1     ContainerCreating   0          3s
```

Note the `STATUS`, and then a couple of seconds later:

```
$ kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
hi-deployment-5f7b895fd9-5hb5h   1/1     Running   0          8s
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          8s
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          8s
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          8s
```

You just specified in 1 lousy file to bring up 4 instances of your app and here
we are. Let's verify that these pods have indeed the label `app=hi` so they're
under management of the `Deployment`:

```
$ kubectl get pods --show-labels
NAME                             READY   STATUS    RESTARTS   AGE     LABELS
hi-deployment-5f7b895fd9-5hb5h   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
```

Yep, there we have it, `app=hi`. We said the Deployment specifies that at all
times there needs to be 4 replicas. So what if we delete one?  For example,
delete the first pod:

```
$ kubectl delete pod hi-deployment-5f7b895fd9-5hb5h
```

Then check your pods, with label `app=hi` (you can use the `-l` flag to specify to only see pods with that label):

```
$ kubectl get pods -l app=hi
NAME                             READY   STATUS    RESTARTS   AGE
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          6m6s
hi-deployment-5f7b895fd9-h7mz7   1/1     Running   0          82s
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          6m6s
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          6m6s
```

Still 4 pods! But if you look at the `NAME`s you'll see that
`hi-deployment-5f7b895fd9-5hb5h` is indeed gone, and replaced by the new pod
`hi-deployment-5f7b895fd9-dt8kf`. The `Deployment` indeed guaranteed that 4
replicas are running at all times. The Zombie Apocalypse is nigh.

You have those 4 pods running, but you can also get more info on the `Deployment` resources running right now as well:

```
$ kubectl get deployment
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
hi-deployment   4/4     4            4           8m48s
```

Note that it indicates that `4` out `4` pods are ready. In this article, I'll
stay at the abstraction of `Deployment` and `Pods` but if you do the following,
you'll see that there's something called a
[`ReplicaSet`](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
created as well:

```
$ kubectl get replicaset
NAME                       DESIRED   CURRENT   READY   AGE
hi-deployment-5f7b895fd9   4         4         4       10m
```

It's actually this `ReplicaSet` that is created by the `Deployment` that makes
sure that the replicas are kept to `4` (see the `DESIRED` column). The
`Deployment` does not create the pods directly. I'll not go into more detail
around `ReplicaSet`s as you'll usually not deal with them directly.

![Running A Deployment on laptop](./diagrams/deployment.svg)

Note how we referred explicitly to 4 instances of the application, not 4 nodes.
The 4 pods running the containers are all running on the same node in your
example. Of course, in reality, you'll have more than 1 node to run your
application, and your application instances may have been scheduled on
different nodes to accomoadate for, for example, CPU requirements your
application needs: you may want to run 100 instances, but your node is a
machine that could not run 100 instances, you may need 10 nodes.  

# Clients for Your Application

Recall the 2nd requirement we listed in the Introduction:

!!!
   Clients of your application will want to connect to one address, requests
   will be guided to different instances of your application automatically

## (Internal) Clients on the Same Cluster

Let's first consider clients that want to connect to your application with the clients on the same cluster. Why is this tricky?

- Your applications runs on several pods, each pod having their own IP. For a client to connect to a pod, it needs to know the IP, but since pods are so-called _ephemeral_ (they could be removed when a node fails etc), that IP may change
- There are multiple instances of your application running (4 in our case), so the client would need to know all 4 IPs and select one

How can we avoid that a client needs to know that list of ever-changing IPs? We
need another kubernetes resource of course: a _Service_. 

A _Service_ will provide you with 1 IP and will <a
							href="https://en.wikipedia.org/wiki/Load_balancing_(computing)">load-balance</a>
requests to that IP by redirecting to the pods that are able to serve the
request. As with _Deployments_ before, question is how will the Service know
what Pods it controls? Labels! Remember that all of the pods in our current
deployment have label `app=hi`. We can define a _Service_ that provides the IP
and load-balancing for exactly those pods, create a file `hi_service.yaml`:

```
apiVersion: v1
kind: Service
metadata:
  name: hi-service
spec:
  ports:
    - port: 80
      targetPort: 8111
  selector:
    app: hi
```

By now you're used to seeing these. So we define a service (`kind: Service`),
we define a name for the service (`hi-servive`) and we specify that we're
forwarding port `80` of this service to port `8111` of the contaner (if you
scroll up, you'll see that `8111` is indeed the port our `hi` app is listening
on). Finally, we specify using a `selector` that this service is a service
fronting pods with label `app=hi`. 

You can create the service as usual:

```shell
$ kubectl create -f hi_service.yaml
```

And then verify it's there:

```shell
$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
hi-service   ClusterIP   10.105.43.171   <none>        80/TCP    7s
```

Note the `CLUSTER-IP`: the servive specifies an IP local to your cluster: all
requests to this IP (port 80, as specified) will be routed to 1 of the pods
with label `app=hi` on port `8111`. We nowhere specified that IPs of all the
individual pods, so we don't keep a list. One extra thing to note is that we do
not see an `EXTERNAL-IP`. Indeed, this IP is only available to clients within
the cluster. I'll show in the next section how to make the service available
outside the cluster.

![A Service](./diagrams/deployment.svg)

From that picture, note that the deployment serves a different function than
the service. The deployment regulates the number of replicas of the pods,
whereas the service's purpose is to provide 1 interfacing IP for clients that routes to the IPs of the endpoints. In fact, you can see the latter by doing:

```shell
$ kubectl describe service hi-service
Name:              hi-service
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=hi
Type:              ClusterIP
IP:                10.105.43.171
Port:              <unset>  80/TCP
TargetPort:        8111/TCP
Endpoints:         172.18.0.6:8111,172.18.0.7:8111,172.18.0.8:8111 + 1 more...
Session Affinity:  None
Events:            <none>
```

Note the `Endpoints`: those are the IPs of your pods (with the port `8111` on
which your container runs).

So far we've highlighted that this allows clients _from within the cluster_ to
hit your service at your cluster IP `10.105.43.171` and be routed to the different pods. How do we test this? How
do we get a client _from within the cluster_?

One way to get _in the cluster_ is using `exec` which executes a command of
your choice on a pod of your choice. So if get into a pod, we'll be clearly _in
our cluster_. What command would you want to execute to test your cluster IP?

You'd probably want something like `curl 10.105.43.171`, so you'd do:

```shell
$ kubectl exec hi-deployment-5f7b895fd9-dt8kf -- curl 10.105.43.171
```

Note that you picked a pod `hi-deployment-5f7b895fd9-dt8kf` by looking at
`kubectl get pods`. The `--` separates your `kubectl` command-line arguments
from the command you're executing (which starts with `curl..`). If you execute
this command, you'll get an error:

```
\"curl\": executable file not found in $PATH"
```

Stepping back you'll then noticate that you based your container image on the
small Linux image `alpine` and `curl` does not come installed with that Linux.
What does come installed that you could use:
[`wget`](https://en.wikipedia.org/wiki/Wget).

```shell
$ kubectl exec hi-deployment-5f7b895fd9-dt8kf -- wget -q -O - 10.105.43.171
```

Execute that a few times and see subsequent outputs:

```
Hi from hi-deployment-5f7b895fd9-r58kc
Hi from hi-deployment-5f7b895fd9-wgwpp
Hi from hi-deployment-5f7b895fd9-h7mz7
```

You're hitting 1 IP `10.105.43.171` but you see that it it's indeed the
different instances of your application (on different pods) answering the call
with those different hostnames. Magic!

## (External) Clients

Quite some hoop-jumping happened in the previous section to access our app
through the service, as the service's IP is local to the Cluster. In reality,
that won't always cut it of course: you'll want external clients to have access
to your application (like a user's browser for example).

There are several ways one can accomplish this, but we'll focus on _creating an
[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)
resource_. If the word _ingress_ throws out, check out this human
[definition](https://www.thefreedictionary.com/ingress). There are not a lot of
better ways to make something seem complicated, but calling an _entrance_, an
_ingress_ is definitely one of them.

TODO: show yaml for ingress, describe it in detail, do a `kubectl get ingresses`

TODO: explain that you'll edit `/etc/hosts` (in real world you'd do this with your provider)

TODO: now do curl or open browser at URL `hi.stijnheymans.net`

TODO: explain flow with diagram


# Conclusion

We did not cover a whole bunch, such as volumes, secrets, jobs, cron jobs, namespaces etc etc

# Endnotes

[^sidecar]: We often refer to containers that always run alongside your _main_ container as _sidecars_ for obvious non-inspired reasons.

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-12447521-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'UA-12447521-1');
</script>

<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

