<meta charset="utf-8">
                            **Elements of Kubernetes**
                            Stijn Heymans
                            published: N/A

# Introduction

In this article, I will go over the elements of Kubernetes _from an application
developer's perspective_. Familiarity with deploying applications in
a production environment is expected[^familiarity].


Rather than explaining Kubernetes from the ground-up and building a pie in the
sky, I'll introduce Kubernetes as something that provides solutions for
problems you are encountering in high-traffic production
environments right now:

- Your application needs to serve 1000s of requests per second so you will need many instances of the application running
- Clients of your application need to connect to one IP address and requests will be guided to those different instances of your application automatically
- You need to be able to push changes to your application without downtime
- You need to be able to rollback changes to your application
- Your application needs to be _warmed up_: things need to happen before your application can start serving up requests. For example, the application instance needs to sync its data with the data of other instances
- When an instance of your application dies (and is thus no longer able to serve requests), that instance needs to become invisible to clients of your application. Traffic should be guided to still functioning instances of your application
- To save costs, for example, during the night when you need less capability, you'll want to reuse some of your machines that the instances of application normally run on to run batch jobs for maintenance
- You will have regular period in the year during which the amount of machines to serve your traffic will not be enough (during a promotion, or, say, the holiday period), so you want to scale up the amount of physical machines you have available. After the busy period, you want to get back to normal and scale down

It will come as no suprise (if you _are_ surprised: Sinterklaas does indeed
exist _and_ the world is a beautiful place where no-one drags themselves
through wondering _where and when in the name it all went wrong_) that
_Kubernetes_ is the _thing_ that will provide solutions for those problems.
I'll refer to _Kubernetes_ by its common abbreviation of _k8s_ -- pronounced
_kates_, because that's what all the cool kids do and I'll do the same to just
to spite them with my lack of cool- and kidness. While Kubernetes is massive
hammer, it does hammer the above problems squarely away.  I disparagingly say
_hammer_ as I say almost everything disparaginly but also because k8s is able to
do more than what you'd need to solve the above stated problems.  Blame that on
its historic origin as used by a Cloud Provider in which many different
applications need to run on the same cluster of machines in a desperate capitalistic attempt
to optimize usage of infrastructure.  Given the problems we laid out, we are
not _that_ interested in running different applications on the same
infrastructure, although the particular requirement to save cost during the
night and run some batch jobs on the same machine that your machine runs on
during the day, is designed to hint at that use.

The requirements as laid out above come from my personal experience, and my
limited knowledge I have on k8s comes from [Kubernetes in
Action](https://www.goodreads.com/book/show/34013922-kubernetes-in-action) by
Marko Lukša, which gave me all the misplaced confidence to play an expert on
the Internet. The book goes in tremendous depth and is useful for
infrastructure administrators (which is a whole world on itself that I will
barely touch on here). In this article, I'll mostly stick to what I know which
is application development and we'll go over every single one of the
problems/requirements and discuss how k8s helps solving them.

XXRev1

# Your Application and Running Many Instances of It

!!!
   Your application needs to serve 1000s of requests per second so you will need many instances of the application running

## A Containerized Application

For the application, I'll be running a simple web server that serves up the
sentence `Hi from HOST` where `HOST` will be the host it is running from. We're
in a brave new world, so this application has been packed up such that it can
run as a Docker container.

I've posted the source of this application on
[github](https://github.com/sheymans/elementsOf/tree/master/kubernetes/demo/hi_app)
and I've pushed the container image to the [Docker
Hub](https://hub.docker.com/) with name
[stijnh/hi_app](https://hub.docker.com/repository/docker/stijnh/hi).

If at this point, you feel slightly lost and it's related to this article, this
may be a good time to take a break from this article and read up on the
[Elements of Docker
Containers](http://www.stijnheymans.net/elements_of_docker_containers.html).

The container image is public so you can try out the app on your laptop right now with:

```
docker container run -d -p 8111:8111 -t stijnh/hi
```

Recall that `docker container run` indicates you're asking to run a container
image; `-d` indicates you're going to do that in the background as a daemon
(rather than interactive), `-p 8111:8111` means you're mapping your `localhost`
port 8111 (on your laptop) to port `8111` on your docker container behind which
your app is running, `-t stijnh/hi` indicates the image you're using to get
the container from.

At this point you can open your browser, and navigating to `localhost:8111`
should show you `Hi from fbd698569ec5` where `fbd698569ec5` will be different
of course for you as this the container's hostname.

Alright, at this point, I'll refer to this state as "you have 1 app
instance running". It's running as a container on your laptop. Your laptop in
this sense is the _machine_ (or if you feel like a rockstar, you're _bare metal
machine_).

## Running Many Instances of It

### Pod

We have 1 instance of our app running, but now you'll have thousands of users
saying hi, so you can't have only 1 instance -- that will not fit your
megalomaniac dreams!  We need more! We need at least 3!

But first we need to define a bit better of what we need 3. We clearly need 3
containers (each container runs our app). As this is example is illustrating a
couple of elements of k8s, we've not mentioned real-world things like
logging. Typically, your application will be writing logs, and you'll need
something to _rotate_ these logs away to more permanent storage (to [AWS's S3](https://aws.amazon.com/s3/)
for example, where you could query it using
[Athena](https://aws.amazon.com/athena/)). We'd want this log writing in a
separate container (other apps could use it as well so it's not typical for
this app alone). You'd want these 2 containers always running
together[^sidecar], so if I say I want 3 instances of my app, I actually want 3
instances of my app with the logging container. Hence, the first level of
abstraction -- the base unit it deals with -- is _not_ a container, it's
something called a _pod_ in k8s. So let's do some more wrapping, and
wrap that container into a _Pod_ manifest[^manifest] that describes what the
pod should look like. If you recall `Dockerfile`s, then this is similar in
philosophy: in a `Dockerfile` you describe your app so that `docker` knows how
to build an image, in a _Pod_ manifest you describe what all goes in your
_Pod_, most importantly what container image to use such k8s knows how to
create a Pod.

Pods are usually defined in [YAML](https://en.wikipedia.org/wiki/YAML) files,
so let's do this. I have a file `hi-pod.yaml`:

```yaml
apiVersion: v1
kind: Pod
metadata:
  name: hi-pod
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

which is really doing nothing more than naming my pod `hi-pod` and indicating
that it's running the container
[`stijnh/hi`](https://hub.docker.com/repository/docker/stijnh/hi).

Let's go over the pod definition, line by line:

```
apiVersion: v1
```

Indicates what API version of k8s this k8s _resource_ is defined
in (_Pods_, together with a whole lot of other things, are named _resources_ in
k8s), in this case `v1`. I have no memory, nor patience for remembering API
versions, but there are [explanations of what version to use when out
there](https://matthewpalmer.net/kubernetes-app-developer/articles/kubernetes-apiversion-definition-guide.html).



```
kind: Pod
```

The type of resource you're defining, a `Pod`.

```
metadata:
  name: hi-pod
```

Some metadata about the pod, in this case its name `hi-pod`.

```
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

The pod's specification, in this case a list of 1 container, where the
container is described by it's image tag `stijnh/hi`, given a name `hi`, and
indicating that the container exposes the port `8111`.

If you only remember one thing from the above, may it be that you'll see 4
parts of such a resource description which you'll see with other types of
resources as well: the `apiVersion`, the `kind`, the `metadata`, and the
`spec`.

With that said, can we deploy this pod now please? Yes. I'm going to assume
you're on your laptop and have something like
[minikube](https://github.com/kubernetes/minikube) installed. As I'm tackling
k8s from the perspective of an application developer, I'll not get into the
details of setting up a cluster that serves production traffic. 

This _is_ a good time to to talk about some general k8s concepts. But first, go
check out [minikube](https://github.com/kubernetes/minikube) and install it so
you can try out some stuff on your laptop.

Once you're back (you were never gone though, were you), you might have read
that minikube will give you a _single-node cluster_. You'll hear _cluster_ a
lot, so what is a cluster? My working definition is that it's a set of nodes.
What's a _node_ then? Again, my working definition is _that's a machine, an EC2
instance, a laptop, some kind of computor with a CPU and memory, an old Pentium
tower in a dusty basement, ..._ you get the idea. A typical mapping in my head
would be _if I have 40 EC2 instances in the EU region to serve traffic_, that's
a _cluster_ of _40_ nodes. 


Now that you have `minikube` installed, this command should show something:

```shell
$ kubectl get nodes
NAME       STATUS   ROLES    AGE   VERSION
minikube   Ready    master   0d   v1.18.3
```

Your first use of `kubectl`, but not your last. `kubectl` is the command-line
tool that will allow you to interact with your cluster asking it about its
nodes, its pods, ...In this case, we know that we 1 node running, called
`minikube`. This is good to keep in mind, all example we'll show later will
involve 1 node. In reality, you'll be working with clusters that have many many
more nodes of course.

Alright, we have a cluster with 1 node. We have our `hi` app, tucked away in a
container, and we defined a pod that is supposed to run that container. Let's
deploy that pod using `kubectl`:

```shell
$ kubectl create -f hi_pod.yaml
pod/hi-pod created
```

Verify that your pod is created:

```shell
$ kubectl get pods
NAME     READY   STATUS    RESTARTS   AGE
hi-pod   1/1     Running   0          54s
```

This shows us that the pod is `Running` and has been doing so for `54s`. Note the levels of indirection: 

- when we ran the app `hi.js` we could just navigate to `localhost:8111` and see the `Hi from...`
- when we ran the app in a container, we needed to forward the localhost's port `8111` _into_ the container using `docker container run -d -p 8111:8111 -t stijnh/hi`
- when we run the app in a container in a pod, we need to do what?

We could forward ports via `kubectl` as well, but that would be mostly be for
debugging. We'll defer to answering this question to the next section where
we'll talk about how to expose your pods to other pods. For now, I'll just show how you can check the logs of that pod:

```
$ kubectl logs hi-pod
hi: listening on port 8111 of host hi-pod
```

That's indeed what the app logs. Note that the host is listed as `hi-pod` which
corresponds to the name of the pod.


You can more details on your pod by doing a `describe`:

```shell
$ kubectl describe pod hi-pod
```

There will be a lot of output, but interesting is for example the last section:

```shell
Events:
  Type    Reason     Age        From               Message
  ----    ------     ----       ----               -------
  Normal  Scheduled  <unknown>  default-scheduler  Successfully assigned default/hi-pod to minikube
  Normal  Pulling    23h        kubelet, minikube  Pulling image "stijnh/hi"
  Normal  Pulled     23h        kubelet, minikube  Successfully pulled image "stijnh/hi"
  Normal  Created    23h        kubelet, minikube  Created container hi
  Normal  Started    23h        kubelet, minikube  Started container hi
```

You'll see the container image `stijnh/hi` being pulled and then the container being created. Summarizing this with a picture, we have our container image, deployed in a pod on 1 node:

![Running 1 pod on your laptop](./diagrams/pod.svg)


### Deployment

Now that we have 1 instance of your application running (on 1 pod, in 1
container). We indicated as a requirement that _your application to be able to
serve traffic you have, which runs into the 1000s of request per second_ and
turns out 1 instance of your application is not able to do that. Assume each
application can serve 250 requests per second, and we need to be able to serve
1000 in total. Hence, we need 4 instances of the application running.

''OK, let me run 4 containers of my application'', you say. "yes, but no" I
reply. The unit of operation when your application is managed by k8s is a pod,
so we'll want 4 pods (each in turn running 1 container).

There's a k8s resource type (the first one we met was a `Pod`) that
allows you to specify how many pods k8s should be keeping around: a
[`Deployment`](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/). 

So that will be the core functionality of a `Deployment` that I'm interested
in at this point, specify how many of a certain pod I'd like around. As with
pods, deployments are specified using YAML manifests and the line that will get
me my 4 pods is:

```
spec:
  replicas: 3
```

Glad to see you're awake Grasshopper, yeah 4 of course:

```
replicas: 4
```

How will you describe of _which pod you want 4 replicas off_. Kubernetes uses
_labels_ to match these. Essentially, you make sure your pods are labeled, and
then you indicate in your `Deployment` what pods you're describing. An example
of a label is `app=hi`, another could be `tier=dev`, or `color=blue`. The
latter is obviously no good, `color=yellow` would be better. Labels are
so-called key/value pairs, and you can have multiple different labels on a pod
(but you could not have 2 labels with the same _key_: so `color=blue` and
`color=yellow` would not be work.

On the `Deployment` side, you'd then say I want 3 replicas of all pods matching
_these labels_. For example, you'd indicate that you want all pods with label
`app=hi`:

```
spec:
  selector:
    matchLabels:
      app: hi
```

Easy enough. "But but, we did not give our pod this label, so there's be
nothing to find". Yes, The final piece of such a `Deployment` would be to see
what kind of pods you want to create, a so-called _pod template_:

```
spec:
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
      - image: stijnh/hi
	name: hi
        ports:
          - containerPort: 8111
```

The latter looks exactly like our pod definition earlier, except that it seems
to also specify that the pod needs to have a label `app=hi` as metadata. Yes,
indeed! In fact, now that have a deployment you can forget all about that
initial pod definition. This `Deployment` knows all it needs to know to create
your 3 pods.  Let's put it all together:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hi-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: hi
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
        - image: stijnh/hi
          name: hi
          ports:
            - containerPort: 8111
```

It describes a `Deployment` with name `hi-deployment`. It specifies that at all
times there need to be 4 replicas of a `Pod` matching `app=hi`. The `template`
indicates how the `Deployment` will go about creating new pods:, it will run
the container `stijnh/hi` and it will label the pod with `app=hi`. The latter
is important as this ensures that the just created Pod would be indeed managed
by the `Deployment`: imagine if we would just a create a `Pod` with label
`app=yo`, the `Deployment` would never think it reached 4 replicas of pods with
label `app=hi` so it'd keep on creating pods.

Before we try this `Deployment`, delete all resources you have so far, and check after that you have 0 pods running:

```
kubectl delete all --all
kubectl get pods
```

Now, create a deployment using the usual `create` based on the above YAML (`hi-deployment.yaml`):

```
kubectl create -f hi-deployment.yaml
```

And then watch the magic:

```
$ kubectl get pods
NAME                             READY   STATUS              RESTARTS   AGE
hi-deployment-5f7b895fd9-5hb5h   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-dt8kf   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-r58kc   0/1     ContainerCreating   0          3s
hi-deployment-5f7b895fd9-wgwpp   0/1     ContainerCreating   0          3s
```

Note the `STATUS`, and then a couple of seconds later:

```
$ kubectl get pods
NAME                             READY   STATUS    RESTARTS   AGE
hi-deployment-5f7b895fd9-5hb5h   1/1     Running   0          8s
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          8s
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          8s
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          8s
```

You just specified in 1 lousy file to bring up 4 instances of your app and here
we are. Let's verify that these pods have indeed the label `app=hi` so they're
under management of the `Deployment`:

```
$ kubectl get pods --show-labels
NAME                             READY   STATUS    RESTARTS   AGE     LABELS
hi-deployment-5f7b895fd9-5hb5h   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          2m40s   app=hi,pod-template-hash=5f7b895fd9
```

Yep, there we have it, `app=hi`. We said the Deployment specifies that at all
times there needs to be 4 replicas. So what if we delete one?  For example,
delete the first pod:

```
$ kubectl delete pod hi-deployment-5f7b895fd9-5hb5h
```

Then check your pods, with label `app=hi` (you can use the `-l` flag to specify to only see pods with that label):

```
$ kubectl get pods -l app=hi
NAME                             READY   STATUS    RESTARTS   AGE
hi-deployment-5f7b895fd9-dt8kf   1/1     Running   0          6m6s
hi-deployment-5f7b895fd9-h7mz7   1/1     Running   0          82s
hi-deployment-5f7b895fd9-r58kc   1/1     Running   0          6m6s
hi-deployment-5f7b895fd9-wgwpp   1/1     Running   0          6m6s
```

Still 4 pods! But if you look at the `NAME`s you'll see that
`hi-deployment-5f7b895fd9-5hb5h` is indeed gone, and replaced by the new pod
`hi-deployment-5f7b895fd9-dt8kf`. The `Deployment` indeed guaranteed that 4
replicas are running at all times. The Zombie Apocalypse is nigh.

You have those 4 pods running, but you can also get more info on the `Deployment` resources running right now as well:

```
$ kubectl get deployment
NAME            READY   UP-TO-DATE   AVAILABLE   AGE
hi-deployment   4/4     4            4           8m48s
```

Note that it indicates that `4` out `4` pods are ready. In this article, I'll
stay at the abstraction of `Deployment` and `Pods` but if you do the following,
you'll see that there's something called a
[`ReplicaSet`](https://kubernetes.io/docs/concepts/workloads/controllers/replicaset/)
created as well:

```
$ kubectl get replicaset
NAME                       DESIRED   CURRENT   READY   AGE
hi-deployment-5f7b895fd9   4         4         4       10m
```

It's actually this `ReplicaSet` that is created by the `Deployment` that makes
sure that the replicas are kept to `4` (see the `DESIRED` column). The
`Deployment` does not create the pods directly. I'll not go into more detail
around `ReplicaSet`s as you'll usually not deal with them directly.

![Running A Deployment on laptop](./diagrams/deployment.svg)

Note how we referred explicitly to 4 instances of the application, not 4 nodes.
The 4 pods running the containers are all running on the same node in your
example. Of course, in reality, you'll have more than 1 node to run your
application, and your application instances may have been scheduled on
different nodes to accommodate for, for example, CPU requirements your
application needs: you may want to run 100 instances, but your node is a
machine that could not run 100 instances, you may need 10 nodes.  

# Clients for Your Application

Recall the 2nd requirement we listed in the Introduction:

!!!
   Clients of your application need to connect to one IP address and requests will be guided to those different instances of your application automatically

## (Internal) Clients on the Same Cluster

Let's first consider clients that want to connect to your application with the clients on the same cluster. Why is this tricky?

- Your applications runs on several pods, each pod having their own IP. For a client to connect to a pod, it needs to know the IP, but since pods are so-called _ephemeral_ (they could be removed when a node fails etc), that IP may change
- There are multiple instances of your application running (4 in our case), so the client would need to know all 4 IPs and select one

How can we avoid that a client needs to know that list of ever-changing IPs? We
need another k8s resource of course: a _Service_. 

A _Service_ will provide you with 1 IP and will <a
							href="https://en.wikipedia.org/wiki/Load_balancing_(computing)">load-balance</a>
requests to that IP by redirecting to the pods that are able to serve the
request. As with _Deployments_ before, question is how will the Service know
what Pods it controls? Labels! Remember that all of the pods in our current
deployment have label `app=hi`. We can define a _Service_ that provides the IP
and load-balancing for exactly those pods, create a file `hi_service.yaml`:

```
apiVersion: v1
kind: Service
metadata:
  name: hi-service
spec:
  ports:
    - port: 80
      targetPort: 8111
  selector:
    app: hi
```

By now you're used to seeing these. So we define a service (`kind: Service`),
we define a name for the service (`hi-service`) and we specify that we're
forwarding port `80` of this service to port `8111` of the container (if you
scroll up, you'll see that `8111` is indeed the port our `hi` app is listening
on). Finally, we specify using a `selector` that this service is a service
fronting pods with label `app=hi`. 

You can create the service as usual:

```shell
$ kubectl create -f hi_service.yaml
```

And then verify it's there:

```shell
$ kubectl get services
NAME         TYPE        CLUSTER-IP      EXTERNAL-IP   PORT(S)   AGE
hi-service   ClusterIP   10.105.43.171   <none>        80/TCP    7s
```

Note the `CLUSTER-IP`: the service specifies an IP local to your cluster: all
requests to this IP (port 80, as specified) will be routed to 1 of the pods
with label `app=hi` on port `8111`. We nowhere specified that IPs of all the
individual pods, so we don't keep a list. One extra thing to note is that we do
not see an `EXTERNAL-IP`. Indeed, this IP is only available to clients within
the cluster. I'll show in the next section how to make the service available
outside the cluster.

![A Service](./diagrams/service.svg)

From that picture, note that the deployment serves a different function than
the service. The deployment regulates the number of replicas of the pods,
whereas the service's purpose is to provide 1 interfacing IP for clients that routes to the IPs of the endpoints. In fact, you can see the latter by doing:

```shell
$ kubectl describe service hi-service
Name:              hi-service
Namespace:         default
Labels:            <none>
Annotations:       <none>
Selector:          app=hi
Type:              ClusterIP
IP:                10.105.43.171
Port:              <unset>  80/TCP
TargetPort:        8111/TCP
Endpoints:         172.18.0.6:8111,172.18.0.7:8111,172.18.0.8:8111 + 1 more...
Session Affinity:  None
Events:            <none>
```

Note the `Endpoints`: those are the IPs of your pods (with the port `8111` on
which your container runs).

So far we've highlighted that this allows clients _from within the cluster_ to
hit your service at your cluster IP `10.105.43.171` and be routed to the different pods. How do we test this? How
do we get a client _from within the cluster_?

One way to get _in the cluster_ is using `exec` which executes a command of
your choice on a pod of your choice. So if get into a pod, we'll be clearly _in
our cluster_. What command would you want to execute to test your cluster IP?

You'd probably want something like `curl 10.105.43.171`, so you'd do:

```shell
$ kubectl exec hi-deployment-5f7b895fd9-dt8kf -- curl 10.105.43.171
```

Note that you picked a pod `hi-deployment-5f7b895fd9-dt8kf` by looking at
`kubectl get pods`. The `--` separates your `kubectl` command-line arguments
from the command you're executing (which starts with `curl..`). If you execute
this command, you'll get an error:

```
\"curl\": executable file not found in $PATH"
```

Stepping back you'll then notice that you based your container image on the
small Linux image `alpine` and `curl` does not come installed with that Linux.
What does come installed that you could use:
[`wget`](https://en.wikipedia.org/wiki/Wget).

```shell
$ kubectl exec hi-deployment-5f7b895fd9-dt8kf -- wget -q -O - 10.105.43.171
```

Execute that a few times and see subsequent outputs:

```
Hi from hi-deployment-5f7b895fd9-r58kc
Hi from hi-deployment-5f7b895fd9-wgwpp
Hi from hi-deployment-5f7b895fd9-h7mz7
```

You're hitting 1 IP `10.105.43.171` but you see that it it's indeed the
different instances of your application (on different pods) answering the call
with those different hostnames. Magic!

## (External) Clients

Quite some hoop-jumping happened in the previous section to access our app
through the service, as the service's IP is local to the Cluster. In reality,
that won't always cut it of course: you'll want external clients to have access
to your application (like a user's browser for example).

There are several ways one can accomplish this, but we'll focus on _creating an
[Ingress](https://kubernetes.io/docs/concepts/services-networking/ingress/)
resource_. If the word _ingress_ throws out, check out this human
[definition](https://www.thefreedictionary.com/ingress). There are not a lot of
better ways to make something seem complicated, but calling an _entrance_, an
_ingress_ is definitely one of them.

As usual create a YAML, for example, names `hi-ingress.yaml`:

```yaml
apiVersion: extensions/v1beta1
kind: Ingress
metadata:
  name: hi-ingress
spec:
  rules:
    - host: hi.stijnheymans.net
      http:
        paths:
          - path: /
            backend:
              serviceName: hi-service
              servicePort: 80
```

Analyzing this yaml specification, line by line:

```
kind: Ingress
```

indicates that this describes an `Ingress` k8s resource. We have the resource a name:

```
name: hi-ingress
```

And now for the interesting part. We specify a rule that indicates that we want
`hi.stijnheymans.net/` (the root), routed to the service `hi-service` at port
`80`. Recall from the above that we indeed defined such a service `hi-ingress`
(and recall that that service in itself routes requests to any of the pods that
it serves).

Alright, let's put the `Ingress` in play:

```
$ kubectl create -f hi-ingress.yaml
```

Check that it's there:

```
$ kubectl get ingresses
NAME         CLASS    HOSTS                 ADDRESS        PORTS   AGE
hi-ingress   <none>   hi.stijnheymans.net   192.168.64.2   80      10s
```

It's there and it's showing you an IP address that you can hit on (on your
laptop, so an _external_ IP -- not a cluster IP): `192.168.64.2`.

As you've mostly not registered `hi.stijnheymans.net`, let's trick
[DNS](https://en.wikipedia.org/wiki/Domain_Name_System) resolution to think
`hi.stijnheymans.net` resolves to IP `192.168.64.2`. On your laptop, edit
`/etc/hosts` by adding the following line:


```
192.168.64.2 hi.stijnheymans.net
```

Now let's see the magic at work: navigate to
[http://hi.stijnheymans.net](http://stijnheymans.net). You should see `Hi from
hi-deployment-5f7b895fd9-zfb4m` (or something similar). Refresh like the crazy
flower-child you are and feel the glorious greetings from different hosts be
extended to you. This is the smell of success.

Let's draw up how things flow from your enthusiastic refreshing to seeing that `Hi` message:

![External Client](./diagrams/ingress.svg)

We made a simplification to this diagram and left out the [Ingress
controller](https://kubernetes.io/docs/concepts/services-networking/ingress-controllers/):
I don't want to know, I don't need to know, and `Ingress` as a name was worse
enough, making it also a `Controller` is just that one step too far.

# Push Changes without Downtime and Rollback If things go Haywire

We already know the base k8s kubernetes resource that will help us tackle the next 2 requirements:

!!!
   You need to be able to push changes to your application without downtime

!!!
   You need to be able to rollback changes to your application

Recall the original deployment YAML:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hi-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: hi
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
        - image: stijnh/hi
          name: hi
          ports:
            - containerPort: 8111
```

Say you have a 2nd version of the image `stijnh/hi` ready: `stijnh/hi:v2`. How
can I make sure that all instances of my application are running `stijnh/hi:v2`
instead of `stijnh/hi`.

The naive answer would be the correct one: replace `stijnh/hi` in the deployment by `stijnh/hi:v2`:


```
        - image: stijnh/hi:v2
```

That's it. The default strategy for roll-outs that k8s `Deployment`s use is the
so-called `RollingUpdate` strategy. With that strategy, k8s would scale down
your replicas (4 initially) of the pods that run the old image `stijnh/hi` and
scale up the replicas (from 0 to 4) with the new image `stijnh/hi:v2`. This
guarantees that there's no downtime, but it would force you to make sure that
`v2` is compatible to with the first version of your app. Indeed, the service
will start routing requests to new _and_ old pods while we are in the
intermediate state. This may cause problems if you for example did database
changes and `v2` of your app is writing data in a different format than before
and `v1` and `v2` are both reading from that data but `v1` expects the old
format so crashes on that newly written data. Some care is usually warranted to
avoid situations like that (for example, but making sure `v2` does not write
the new format yet, but is able to read both the new and old format). A `v3`
could then subsequently start writing the new format without issues.

If you see no such programmatic way out, but you would be willing to take a
small downtime, you can use the `Recreate` strategy. Your deployment would look
as follows:

```
apiVersion: apps/v1
kind: Deployment
metadata:
  name: hi-deployment
spec:
  replicas: 4
  selector:
    matchLabels:
      app: hi
  strategy:
    type: Recreate
  template:
    metadata:
      labels:
        app: hi
    spec:
      containers:
        - image: stijnh/hi:v2
          name: hi
          ports:
            - containerPort: 8111
```

This will cause k8s to take out all of your `stijnh/hi` pods and immediately
scale up all of the new `stijnh/hi:v2`. This of course will cause a small
downtime, or may result in a situation where while `v2` is scaling up, you do
not have enough pods to serve your production traffic.

To finish this section, some quick commands that allow you to check the deployment while it's happening:

```
kubectl rollout status deployment hi-deployment
```

If something went wrong you can execute a rollback immediately as follows:

```
kubectl rollout undo deployment hi-deployment
```

You may be used to control the rolling update: how much percentage of the old
pods can you take out while bringing in new pods with the latest version. Can
you do that with k8s? Yes, you can it's called [proportional
scaling](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/#proportional-scaling).


# Warm-up Of Your Application

!!!
   Your application needs to be _warmed up_: things need to happen before your application can start serving up requests. For example, the application instance needs to sync its data with the data of other instances

 When a pod gets deployed on a cluster (as part of a `Deployment` for example), it might not immediately be ready for serving traffic. For example, a web app may need some time to initialize or your app may need to pull data from S3 into memory (configuration data for example), or in a distributed setting, your app may need to reach consensus with other instances.
 These are all situations where your service that is routing traffic to pods,
 should not immediately route traffic to that new pod. Only when the new pod is
 _ready_ it gets included as one of the pods that a service routes traffic to.

 The way this is accomplished is through the use of a so-called _readiness
 probe_. There are several ways to define readiness probes, but I'll focus on
 an `HTTP GET` readiness probe: doing a `GET` to the pod will indicate, based
 on the returned status of that `GET`, whether the pod is ready for serving
 traffic or not.

Without going into more detail, you'll add a `readinessProbe` attribute to your
pod specification, for example, you would change the following pod:

```
apiVersion: v1
kind: Pod
metadata:
  name: hi-pod
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
```

to include the `readinessProbe`:


```
apiVersion: v1
kind: Pod
metadata:
  name: hi-pod
spec:
  containers:
    - image: stijnh/hi
      name: hi
      ports:
        - containerPort: 8111
      readinessProbe:
        httpGet:
          path: /readiness
          port: 8888
        initialDelaySeconds: 300
        periodSeconds: 30
```

Once the pod is deployed, we'd wait 300 seconds (`initialDelaySecods`), and
then every 30 seconds (`periodSeconds`), we'd do a `GET` on port 8888 of the
container at path `/readiness`. If you that `GET` returns `200` your pod is
ready and will be included for, for example, routing traffic to from the
service.

This of course requires that your app, defines a path `/readiness` and that it
only returns `200` if it is truly ready. For a web app, this will naturally
follow when the web app is up and running. If there's more complex start-up
going on (like reaching consensus in a distributed setting), you should
guarantee that the `/readiness` path only returns a `200` when the application
is indeed ready.

# Are you alive?

!!!
   When an instance of your application dies (and is thus no longer able to serve requests), that instance needs to become invisible to clients of your application. Traffic should be guided to still functioning instances of your application

Well, that requirement sounds related to the readiness requirement, doesn't it.
In this case, one or more instances of your application died (or starting
spewing errors and no-one knows why, but usually we just reboot the whole fleet
_just be sure_).

If that happens, we want to take the pod with the dead application out of
rotation (it should stop being routed to by the service for example) and we'd
attempt a restart. Similar the above `readinessProbe`, we have a concept called
`livenessProbe`. You'd similarly define for example (for the HTTP case) a `GET`
to a path on your app that indicates whether your app is alive. If that `GET`
indicates it's not alive (for example, no `200` was returned), the pod gets
taken out of rotation and a new pod is restarted.

Definition of a liveness probe is similar to how we define readiness probes (see
above, on the pod), so I'll not spend any more words on it.

# Horizontal Autoscaling

!!!
   To save costs, for example, during the night when you need less capability,
   you'll want to reuse some of your machines that the instances of application
   normally run on to run batch jobs for maintenance

 
Note that this does not talk about getting rid of actual machines/nodes: it
just indicates there are periods that I'll want to downscale the pods I have to
make room on my nodes for other jobs possibly.

You can schedule jobs during the night using k8s resources like [CronJobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/) so that's not the interesting part of this requirement. The core of what I'm interested in this requirement is actually:

!!!
   Can I minimize the amount of pods I need to be able to serve traffic that I need, but of course in a way that I actually am able to serve traffic.

Simplified, during the day I may need 4 replicas of my application, but during
the night I may only need 1. Rather than having all 4 replicas run all the time
and incur costs, can I scale down to the amount of replicas I _need_ and scale
back up similarly?

This question leads first to another question of course: what does _I need_
mean. When should I scale up or down? Well, you can specify that. You could
specify that based on CPU usage (if average CPU usage goes above a threshold,
say 80% for a certain amount of time, spin up extra pods so that that average
will go down again), but you can have other _metrics_ as well. Here I'll talk
about "transactions per second" (TPS).

Let's start with the snippet that would indicate that each pod can handle about
200 requests per second:

```
metric:
  name: transactions-per-second
describedObject:
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  name: hi-ingress
target:
  type: AverageValue
  value: 200
```

Analyzing this line-by-line:

```
metric:
  name: transactions-per-second
```

gives the metric the name `transactions-per-second`, could be `leonardo dicaprio` for all we care. 

```
describedObject:
  apiVersion: networking.k8s.io/v1beta1
  kind: Ingress
  name: hi-ingress
```

indicates the object we're describing, in this we're describing our `Ingress`
`hi-ingress`. Recall that the Ingress is thing that makes our service available
externally. Indeed, rather than describing the traffic on the Pod directly, we just indicate that we'll be referring to the external facing part of our service. 

```
target:
  type: AverageValue
  value: 200
```

then indicates that the `Ingress` should _on average_ be sending 200 requests to each of the pods.

Alright, what should happen when we go over those `200` average requests to the
pods: we should scale up the amount of pods. Is there a limit to scaling up the
pods? Yes, there is. It would indicated when you're defining your so-called
[`HorizontalPodAutoscaler`](https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/)
(HPA). Write the spec up in a YAML `hi-hpa.yaml`:

```
apiVersion: autoscaling/v2beta2
kind: HorizontalPodAutoscaler
metadata:
  name: hi-hpa
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hi-deployment
  minReplicas: 1
  maxReplicas: 5
  metrics:
    - type: Object
      object:
        metric:
          name: transactions-per-second
        describedObject:
          apiVersion: networking.k8s.io/v1beta1
          kind: Ingress
          name: hi-ingress
        target:
          type: AverageValue
          value: 200
```

You'll see the `metric` appear that we just appeared. In addition:

```
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: hi-deployment
```

it refers to our `hi-deployment`. Recall that it is this deployment that tells
k8s what pods look like (and by default makes sure there are 4 pods with the
right labels). You also see 

```
  minReplicas: 1
  maxReplicas: 5
```

Whereas the default is `4` pods, the HPA may scale you down to 1 pod or scale
you up to 5 pods until we hit the target of `200` transactions on average on
the `Ingress`.

Wow, and that's all there is too it. Re-read the last _the HPA may scale you
down..._ and cherish the way you've traveled since starting to read this
article. 

Try it out as usual with:

```
$ kubectl create -f hi-hpa.yaml
```

# Autoscale the cluster

!!!
   You will have regular period in the year during which the amount of machines
   to serve your traffic will not be enough (during a promotion, or, say, the
   holiday period), so you want to scale up the amount of physical machines you
   have available. After the busy period, you want to get back to normal and
   scale down

The HPA we just saw will help you increase and decrease pods on your cluster,
making up space for other applications/batch processes to run on your cluster,
and saving costs in general. However, it will not do anything about the actual
physical nodes in your cluster. You could be having a cluster of 100 nodes with
1 pod running on one of them. Hardly an efficient deal.  Or vice versa, say
your traffic is exploding: trying to scale up from 1 to 100 pods on 1 available
node in your cluster, will lead to nothing but disaster.

In both cases, you'd want to decrease or increase the actual nodes in your
cluster. We're now solidly in the realm of someone more knowledgeable than me,
but I'll give it a desperate attempt. Kubernetes has the concept of a [Cluster
Autoscaler](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler).
When configured for your cluster, it'll make sure nodes are spun up or down
given the requirements of the pods running on it. For example, if the HPA
determines, I need 1000 more pods to be able to meet this `200` average
transactions per second, your cluster would be spinning these up for you.
Configuration of the cluster depends on cluster provider, so see
[here](https://github.com/kubernetes/autoscaler/tree/master/cluster-autoscaler)
for details.

One thing to take into account and nicely explained in [this autoscaling
article](https://www.cncf.io/blog/2019/10/29/kubernetes-autoscaling-101-cluster-autoscaler-horizontal-autoscaler-and-vertical-pod-autoscaler/)
is that scaling up the cluster will take more time than scaling up the pods as
scaling the cluster will require you to ask your cluster provider for actual
physical instances/nodes.

# Conclusion

I tried to give you an elemental overview of the problems k8s is solving for you. As usual, I've not talked about a 1000 other topics, 
such as [volumes](https://kubernetes.io/docs/concepts/storage/volumes/),
[secrets](https://kubernetes.io/docs/concepts/configuration/secret/),
[jobs](https://kubernetes.io/docs/concepts/workloads/controllers/job/), [cron
jobs](https://kubernetes.io/docs/concepts/workloads/controllers/cron-jobs/),
[namespaces](https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/),... 
As mentioned before, the book [Kubernetes in
Action](https://www.goodreads.com/book/show/34013922-kubernetes-in-action) by
Marko Lukša, or the internet itself, if it obliges, will answer all your further questions.

# Endnotes

[^familiarity]: If you can imagine what things like _downtime_, _rollback_, _production_, _client_, and _IP address_ mean, you're OK.

[^sidecar]: We often refer to containers that always run alongside your _main_ container as _sidecars_ for obvious non-inspired reasons.


<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-12447521-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];

  function gtag() {
    dataLayer.push(arguments);
  }

  gtag('js', new Date());

  gtag('config', 'UA-12447521-1');
</script>

<link rel="stylesheet" href="https://casual-effects.com/markdeep/latest/latex.css?">
<!-- Markdeep: --><style class="fallback">body{visibility:hidden}</style><script src="https://casual-effects.com/markdeep/latest/markdeep.min.js?" charset="utf-8"></script>

